---
title:"JMJPFU-XGBoost Model"
output: html_notebook
---

### JMJPFU 
### 2-Feb-2017 : 

This is a sample notebook for XGBoost model. 

### Step1 : Setting the library functions

```{r}
library(xgboost)
library(Matrix)
library(data.table)
library(Rtsne)
```


### Step 2 : Preparing the labels using one hot encoding

XGboost works only with numerical data. Therefore the label data has to be converted using one-hot encoding.

```{r}
xTrainohe <- xTrain
oheLabel <- sparse.model.matrix(label~.-1,data=xTrainohe)
head(oheLabel)
```
Now converting the label data into numeric form.

```{r}
xLabel <- xTrain[,"label"]

levels(xLabel)

num.class <- length(levels(xLabel))

levels(xLabel) <- 1:num.class

head(xLabel,50)
  
  
```

Let us ignore the earlier created sparse matrix and create a new train matrix without including the label in it.

```{r}

xTrainohe <- xTrainohe[,1:11] # Exclusing the labels vector
```

Let us now try out a tSNE ( t-Distributed Stochastic Neighbour Embedding) visualisation 

Reference Link : https://rpubs.com/flyingdisc/practical-machine-learning-xgboost


```{r}
tsne <- Rtsne(as.matrix(xTrainohe),check_duplicates = FALSE,pca = TRUE,perplexity = 30,theta = 0.5,dims=2)

summary(tsne)
```

Visualising the same

```{r}
library(ggplot2)

embedding = as.data.frame(tsne$Y)
embedding$Class = xTrain[,"label"]

g = ggplot(embedding,aes(x=V1,y=V2,color=Class)) + geom_point(size=1.25)+ guides(colour=guide_legend(override.aes = list(size=6))) + xlab("") + ylab("") + ggtitle("t-SNE 2D Embedding of 'Label' Outcome") + theme_light(base_size = 20) + theme(axis.text.x=element_blank(),axis.text.y=element_blank())

print(g)
```

Looking at the 2d embedding we can see some clusters being formed. 

### Generating a random number between 1 & 736

```{r}

rand <- sample(1:736,500)

write.csv(rand,"BibleRead.csv")

```

### JMJPFU
### 3-Feb-2017


1. Continue with the Xgboost model example and wrap it up
2. Start with new methods and models

```{r}
# Convert data into a matrix
xTrainMatrix = as.matrix(xTrainohe)
mode(xTrainMatrix) = "numeric"

# xgboost takes multi-labels in [0,numof classes]
y = as.matrix(as.integer(xLabel)-1)
```

### Step 3.a : Setting the xGBoost Parameters

```{r}
# xgboost parameters

param <- list("objective"= "multi:softprob", # multiclass classification
              "num_class" = num.class, # Number of classes
              "eval_metric" = "merror", # Evaluation metric
              "nthread" = 8, # number of threads to be used
              "max_depth" = 16, # maximum depth of tree
              "eta" = 0.3, # step size shrinkage
              "gamma" = 0, # minimum loss reduction
              "subsample" = 1, # part of data instances to grow tree
              "colsample_bytree" = 1, # subsample ratio of columns when constructing each tree
              "min_child_weight" = 12 # minimum sum of instance weight needed in a child
              )
```

### Step 3.b : 10-fold cross validation

```{r}
set.seed(1234)

nround.cv = 200

system.time(bst.cv <- xgb.cv(param=param, data=xTrainMatrix,label=y,nfold = 10,nrounds = nround.cv,prediction = TRUE,verbose = FALSE))
```

Checking the cv scores

```{r}
head(bst.cv,10)
```
Let us look at the cross validation score and find the index with the minimum multiclass error rate

```{r}
# index of minimum merror

min.merror.idx = which.min(bst.cv$evaluation_log$test_merror_mean)

min.merror.idx

bst.cv$evaluation_log[min.merror.idx]
```

The minimum error rate is for iteration 157 as shown above

### Step 3.c : Confusion matrix

```{r}
pred.cv <- matrix(bst.cv$pred,nrow=length(bst.cv$pred)/num.class,ncol=num.class)

pred.cv <- max.col(pred.cv,"last") # Taking the max accross the columns which is the probability of each class

# Confusion matrix


library(caret)
confusionMatrix(factor(y+1),factor(pred.cv))
```

Thank you Lord for the progress so far in the xgboost model.

### JMJPFU
### 6-Feb-2017

We will pursue with more experiments with xgboost later on. Let us stop the methods for now and then continue with another method of prediction with another algorithm.
