---
title:"JMJPFU-XGBoost Model"
output: html_notebook
---

### JMJPFU 
### 2-Feb-2017 : 

This is a sample notebook for XGBoost model. 

### Step1 : Setting the library functions

```{r}
library(xgboost)
library(Matrix)
library(data.table)
library(Rtsne)
library(caret)
library(dplyr)
```


### Step 2 : Preparing the labels using one hot encoding

XGboost works only with numerical data. Therefore the label data has to be converted using one-hot encoding.

```{r}
xTrainohe <- xTrain
oheLabel <- sparse.model.matrix(label~.-1,data=xTrainohe)
head(oheLabel)
```
Now converting the label data into numeric form.

```{r}
xLabel <- xTrain[,"label"]

levels(xLabel)

num.class <- length(levels(xLabel))

levels(xLabel) <- 1:num.class

head(xLabel,50)
  
  
```

Let us ignore the earlier created sparse matrix and create a new train matrix without including the label in it.

```{r}

xTrainohe <- xTrainohe[,1:11] # Exclusing the labels vector
```

Let us now try out a tSNE ( t-Distributed Stochastic Neighbour Embedding) visualisation 

Reference Link : https://rpubs.com/flyingdisc/practical-machine-learning-xgboost

Reference Link : https://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/


```{r}
tsne <- Rtsne(as.matrix(xTrainohe),check_duplicates = FALSE,pca = TRUE,perplexity = 30,theta = 0.5,dims=2)

summary(tsne)
```

Visualising the same

```{r}
library(ggplot2)

embedding = as.data.frame(tsne$Y)
embedding$Class = xTrain[,"label"]

g = ggplot(embedding,aes(x=V1,y=V2,color=Class)) + geom_point(size=1.25)+ guides(colour=guide_legend(override.aes = list(size=6))) + xlab("") + ylab("") + ggtitle("t-SNE 2D Embedding of 'Label' Outcome") + theme_light(base_size = 20) + theme(axis.text.x=element_blank(),axis.text.y=element_blank())

print(g)
```

Looking at the 2d embedding we can see some clusters being formed. 

### Generating a random number between 1 & 736

```{r}

rand <- sample(1:736,1)

#write.csv(rand,"BibleRead.csv")

sample(1:736,1)

```

### JMJPFU
### 3-Feb-2017


1. Continue with the Xgboost model example and wrap it up
2. Start with new methods and models

```{r}
# Convert data into a matrix
xTrainMatrix = as.matrix(xTrainohe)
mode(xTrainMatrix) = "numeric"

# xgboost takes multi-labels in [0,numof classes]
y = as.matrix(as.integer(xLabel)-1)
```

### Step 3.a : Setting the xGBoost Parameters

```{r}
# xgboost parameters

param <- list("objective"= "multi:softprob", # multiclass classification
              "num_class" = num.class, # Number of classes
              "eval_metric" = "merror", # Evaluation metric
              "nthread" = 8, # number of threads to be used
              "max_depth" = 16, # maximum depth of tree
              "eta" = 0.3, # step size shrinkage
              "gamma" = 0, # minimum loss reduction
              "subsample" = 1, # part of data instances to grow tree
              "colsample_bytree" = 1, # subsample ratio of columns when constructing each tree
              "min_child_weight" = 12 # minimum sum of instance weight needed in a child
              )
```

### Step 3.b : 10-fold cross validation

```{r}
set.seed(1234)

nround.cv = 200

system.time(bst.cv <- xgb.cv(param=param, data=xTrainMatrix,label=y,nfold = 10,nrounds = nround.cv,prediction = TRUE,verbose = FALSE))
```

Checking the cv scores

```{r}
head(bst.cv,10)
```
Let us look at the cross validation score and find the index with the minimum multiclass error rate

```{r}
# index of minimum merror

min.merror.idx = which.min(bst.cv$evaluation_log$test_merror_mean)

min.merror.idx

bst.cv$evaluation_log[min.merror.idx]
```

The minimum error rate is for iteration 157 as shown above

### Step 3.c : Confusion matrix

```{r}
pred.cv <- matrix(bst.cv$pred,nrow=length(bst.cv$pred)/num.class,ncol=num.class)

pred.cv <- max.col(pred.cv,"last") # Taking the max accross the columns which is the probability of each class

# Confusion matrix


library(caret)
confusionMatrix(factor(y+1),factor(pred.cv))
```

Thank you Lord for the progress so far in the xgboost model.

### JMJPFU
### 6-Feb-2017

We will pursue with more experiments with xgboost later on. Let us stop the methods for now and then continue with another method of prediction with another algorithm.

# JMJPFU
### 20-Mar-2017

We will embark on a detailed data science journey document with XGboost. To embark on the journey we will first do some experiments with some methods and then get on to the documentation phase.

### Experiments 1 : Zero variance

```{r}

# Near zero variance experiments

zero.var <- nearZeroVar(xTrain[,1:11],saveMetrics = TRUE)

zero.var


```

So as seen "Dod50" and "con50" variables have near zero variance and therefore can be excluded from the model after some after thought.

### Experiments 2 : Plot of relationship of features with Outcome

# JMJPFU
#### 21-Mar-2017:

Continuing with experiment 2 to analyse the relationship of features with outcome.

```{r}
featurePlot(xTrain[,1:11],factor(y),"strip")


```

As we can see, the data has different distribution for each of the feature. However, we need to do scaling of the variables and see how they look like with the scaled features.


### Data Science Safari : ##################

Now we will start the data science safari and explore the inner meaning of distributions processes and its connections with the model. The below is the path we will tread

I . Data Cleaning
II. Statistical Exploration
  1. Look at the distribution of the raw variables
  2. Understand the process which generated the data
  3. Get an intuition on the probability model which might have generated the data.
  4. Get some descriptive statistics ( Mean, Median, Mode, SD) for each of the variable
  5. Get histograms, density plots etc for each variable.
  6. Get to know the distributions of the variables and compare it with the intuitions we build about the process which generated data
  7. Look at any linearity assumptions of data
  8. Look at the plots of each variable with respect to the response variables
  9. Look at the correlation plots with respect to the variables
  10. Look at the class distribution of the classes. Get clues on any minority sampling techniques to be used
  11. Get intuitions on features that needs to be developed in the context of the business problem.
  
III. Feature Engineering
Iv . Modelling
V. Deployment.

## Safari : Statistical Exploration

Let us now look at the distribution of the raw variables of multiple samples

The process to do this is as follows
1. Filter out the data set of a battery
2. Create multiple histogram for each variable
3. Study the distributions of each variable accross multiple samples.
4. Write down the intuitions

```{r}
# Filtering out the battery

expBat <- unique(batConsolidated$Battery)[1]
expBat

# Now filtering out the relevant data for this battery

batData <- batConsolidated %>% filter(Battery == expBat)

# Now filtering out the relevant variable data

voltData <- batData %>% filter(measure == "Voltage")
curData <- batData %>% filter(measure == "Current")
conData <- batData %>% filter(measure == "Conductance")
voltempData <- batData %>% filter(measure == "Volttemp")
charData <- batData %>% filter(measure == "Charge" )
disData <- batData %>% filter(measure == "Discharge")
dodData <- batData %>% filter(measure == "DOD")

# Now to viaulise the data

par(mfrow = c(4,2))
a = hist(voltData$Variable, main = "Voltage");plot(a);abline(v=mean(voltData$Variable),col="red")
b= hist(curData$Variable, main = "Current");plot(b);abline(v=mean(curData$Variable),col="red")
c= hist(conData$Variable, main = "Conductance");plot(c);abline(v=mean(conData$Variable),col="red")
d = hist(voltempData$Variable, main = "NormVoltage");plot(d);abline(v=mean(voltempData$Variable),col="red")
e = hist(charData$Variable, main = "Charge" );plot(e);abline(v=mean(charData$Variable),col="red")
f = hist(disData$Variable, main = "Discharge");plot(f);abline(v=mean(disData$Variable),col="red")
g = hist(dodData$Variable, main = "DOD");plot(g);abline(v=mean(dodData$Variable),col="red")

```

```{r}
unique(batData$measure)
```



  
