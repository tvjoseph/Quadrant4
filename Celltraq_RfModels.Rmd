
# JMJPFU
# 27-Jan-2017
# Random Forest Implementation for Celltraq

### Reference Link

http://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/

Search keyword in Google : Random Forest tuning with Caret

First we will try Random Forest Models using the caret package methods. After that we will use Random Forest with the native methods.

## Random Forest Implementation with the Caret Package Methods


### Step A : Loading the required libraries

```{r}
library(plyr)
library(dplyr)
library(caret)
library(ggplot2)
```


### Step 1. Preparing and Preprocessing the training and validation sets

```{r}
set.seed(7)

# Preprocessing 

xTrain <- batDataset1

xVal <- validationSet1

preprocessParamsTr <- preProcess(xTrain,method = c("BoxCox"))

xTrain <- predict(preprocessParamsTr,xTrain)

preprocessParamsVl <- preProcess(xVal,method=c("BoxCox"))

xVal <- predict(preprocessParamsVl,xVal)



```

### Step 2. Tuning the Random Forest Model with traincontrol and grid expansion - Baseline

The parameters which we will tune are the following

1. mtry : Number of variables randomly sampled as candidates for each split
2. ntree : Number of trees to grow

For the baseline we will try the following

mtry = floor(sqrt(ncol(xTrain)))
ntree = 500



```{r}
# Setting the traincontrol parameters
trainConRf <- trainControl(method = "repeatedcv",number=10,repeats=3)

seed = 7

metric = "Accuracy"

set.seed(seed)

mtry = floor(sqrt(ncol(xTrain)))

# Setting the Grid Expand

gridRf <- expand.grid(.mtry=mtry)

rfDefault <- train(label~.,data=xTrain,method="rf",metric=metric,tuneGrid=gridRf,trControl=trainConRf)

print(rfDefault)



```
Let us now try the prediction of the default RF model

```{r}
xVal$rfDefault <- predict(rfDefault,newdata = xVal[,1:11])

confusionMatrix(xVal$label,xVal$rfDefault)
```
Just trying out on the batTestSet also

```{r}
batTestSet$rfDefault <- predict(rfDefault,newdata = batTestSet[,5:15])

table(batTestSet$rfDefault)
```

## The git repository is Quadrant4. The file is picked from the git repository Quadrant4

## JMJPFU
### 30-Jan-2017
1. Let us first check the results we got 
1. Continue with the tuninig and prediction of the RF model as per the ML mastery website

### Step 3 : Testing the results of prediction through visualisation.

```{r}
# Getting those batteries which are indicated as failed

failedBats <- unique(batTestSet %>% filter(rfDefault != "NormalPeriod") %>% select(Battery))

```

Visualising the batteries

```{r}

Temp_bat_consol <- bat_newfeat5 %>% filter(Battery %in% failedBats$Battery[12]) %>% filter(measure %in% c("Conductance","DOD","Voltage")) #  %>% filter(Date <= "2015-05-02") # Taking the requuired data # ,"DOD","Voltage"

q5 <- ggplot(data=Temp_bat_consol,aes(as.factor(Date1),Variable,color=measure)) + geom_point() + facet_grid(measure~Battery,scales = "free") # ,margins =TRUE can be included if we want everything together in one graph

q5 + theme(axis.text.x=element_text(angle=70,hjust=1))


batTestSet %>% filter(Battery %in% failedBats$Battery[12]) %>% select(rfDefault)



```


So as seen from the visualisations, the predictions were reasonably oK. Let us now continue with the other methods as per the website.

### Step 4.a : Tuning the parameter with Random search

Let us search for the most optimal mtry by randomly searching and repeated cross validation

```{r}
# Random Search

control <- trainControl(method="repeatedcv",number=10,repeats=3,search="random") # Setting the train control

set.seed(seed) # setting the seed

mtry <- sqrt(ncol(xTrain)) # Defining the mtry

rfRandom <- train(label~.,data=xTrain,trControl=control, method="rf",metric=metric,tuneLength=15)

print(rfRandom)

plot(rfRandom)



```

Now that we have done the training, let us try prediction on the validation set and check the model.

```{r}
xVal$rfRandom <- predict(rfRandom,newdata = xVal[,1:11])

confusionMatrix(xVal$label,xVal$rfRandom)

```

The model which we got after the random search was pretty neat. Let us try predicting on the test set

```{r}
batTestSet$rfRandom <- predict(rfRandom,newdata = batTestSet[,5:15])

table(batTestSet$rfRandom)

```
Thank Lord, this looks pretty neat. Let us now try the next method, which is Grid Search

### Step 4.b : Grid Search

In this method, we define a grid of parameters to try

```{r}

# Defining the control parameters

control <- trainControl(method = "repeatedcv",number=10, repeats=3,search = "grid")

# Setting the seed

set.seed(seed)

# Defining Tune grid

tunegrid <- expand.grid(.mtry=c(1:10))

# Running the model

rfGrid <- train(label~.,data=xTrain,method="rf",metric=metric,trControl=control,tuneGrid = tunegrid)

# Printing the results

print(rfGrid)

plot(rfGrid)

```
### Tomorrow

1. Continue with other methods as listed in RF.
2. Start off with a new algorithm
