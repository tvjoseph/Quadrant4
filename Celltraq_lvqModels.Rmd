---
title: "JMJPFU-LVQ"
output: html_notebook
---

# JMJPFU
### 16-Feb-2017

This notebook is for demonstrating the Learning Vector Quantization ( LVQ) algorithm. We will use caret for doing this

### Initial tasks : Loading libraries

Let us first load the library functions

```{r}
library(caret)
library(dplyr)
library(ggplot2)
```

### Step 1 : Setting the train control parameters and training the model

```{r}
# Preparing the training scheme
trainCon <- trainControl(method="repeatedcv",number=10,repeats=3)
# Training the model

lvqModel <- train(label~.,data=xTrain,method='lvq',trControl=trainCon,tuneLength=5)

# Summarizing the model

print(lvqModel)


```

Let us try to predict on the validation set with this new model

```{r}
xVal$lvqpred <- predict(lvqModel,newdata = xVal[,1:11])

confusionMatrix(xVal$lvqpred,xVal$label)
```
The initial model is not that promising. The sensitivity values are pretty bad. Let us try tuning the model with grid values

## Step 2 : Finding the parameters with Grid search

```{r}
set.seed(7)

trainCon <- trainControl(method="repeatedcv",number=10,repeats=3)

Grid = expand.grid(size=c(50,60,70,80,90,100), k = c(5,9,10,13,16,18,22))

lvqModel2 <- train(label~.,data=xTrain,method="lvq",trControl=trainCon,tuneGrid = Grid)

print(lvqModel2)
```
With the latest values of LVQ parameters let us try predicting on the validation set

```{r}
xVal$lvqpred2 <- predict(lvqModel2,newdata = xVal[,1:11])

confusionMatrix(xVal$lvqpred2,xVal$label)
```
This model with grid search did not yield any promising results. 

# Tomorrow
1. Let us try new models NB, SVM, Adaboost,GBM etc

