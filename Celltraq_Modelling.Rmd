---
title: "JMJPFU-Celltraq Models"
output: html_notebook
---

# JMJPFU
# 5-Jan-2016

This notebook will detail the modelling steps and process. The broad process is as listed below.

1. Model Exercise 1 : This will be the benchmark model and will not have any preprocessing or transformation. However other activities like spot checking and model tuning will be done.
2. Model improvement 1 : In this model, we will try to do different types of preprocessing and transformation and then check the improvements which will be made in the model if any. Will also try out the minority sampling with SMOTE



# Initial tasks

Loading libraries

```{r}
library(dplyr)
library(ggplot2)
library(caret)
```

Creating the consolidated data set and then ordering the variables

```{r}

# Taking the normal data set

normalTrain <- normalCon %>% filter(label != "To_test")

modelData <- rbind(normalTrain,failCon)

modelData <- modelData %>% select(Battery,Counter,Bench1,Bench2,featDischarge,conSlope,dodMin,Dod50,Dod85,Dodtop,conDrop,contop,con80,contop,voltSD,label)

```

# Step 1 : Creating a Validation Dataset

First let us create a validation dataset and a training data set.

```{r}
set.seed(7)

# Selecting 20% of data for validation

validationIndex <- createDataPartition(modelData$label,p=0.80,list=FALSE)

# Creating the validation set

validationSet <- modelData[-validationIndex,]

# Creating the remaining 80% of data

batDataset <- modelData[validationIndex,]


```

# Step 2 : Descriptive Statistics

Let us carry out some descriptive statistics of the dataset

# Dimensions of the dataset

```{r}
dim(batDataset)
```

# Datatypes

```{r}
sapply(batDataset,class)
```

So we can see that label is a character. We need to convert it into a factor. Let us first look at the various lables in the batDataset

```{r}
unique(batDataset$label)
unique(modelData$label)
unique(validationSet$label)
```

# Changing class of the labels
We can also see that the validationSet has not received representation of all classes. This is probably because the label was not made as a factor. Let us first make the label as a factor and then re-compute the validation sets.


```{r}

modelData$label <- as.factor(modelData$label)

validationIndex <- createDataPartition(modelData$label,p=0.8,list=FALSE)

validationSet <- modelData[-validationIndex,]

batDataset <- modelData[validationIndex,]

unique(batDataset$label)

unique(validationSet$label)

```
Still the validationSet does not have all the classes. So in the next pass we will try minority sampling techniques and redo the validation steps.

Let us now look at the summary statistics of the data. Also look at the distribution of some of the values for Dod50 and contop

# Summary Statistics

```{r}
summary(batDataset)

table(batDataset$Dod50)

table(batDataset$contop)
```

# Distribution of classes

Let us also look at the distribution of classes. This is to check how the imbalance of the classes are

```{r}
cbind(Freq = table(batDataset$label),prop = prop.table(table(batDataset$label))*100)
```

so as seen from the imbalance data the periods before failure are in the minority. This will definitely lead to misclassification and sensitivity will tend to be low. However we will will continue with this distribution for the time being and think of minority sampling in the next pass.

# Looking up at NA values

Let us now check the completeness of the dataset

```{r}

nrow(batDataset)

nrow(batDataset[complete.cases(batDataset),])
```

The above code was to compare between the number of rows of the real data set and also the number of rows after executing the complete.cases on the dataset.

So as seen there are no missing values in the dataset, which is good

# Looking at correlation between data.

Let us see if there are any major correlations between the data points

```{r}
cor(batDataset[,5:15])
```

So as seen, the major correlation is between the Dod85/Con85 and DodTop/ConTop. What is seen is some negative correlation which means that higher the Dod85 lower would be the DodTop. This makes sense. How this correlation will affect the model quality is yet to be seen. 

# Step 3 : Unimodal Data Visualizations.

Let us now look at some visualisation of the variables to understand the skewness and normality of the datasets.

# histograms for each variable

```{r}
par(mfrow= c(3,4))

for(i in 5:15){
  
  hist(batDataset[,i],main=names(batDataset)[i])
}
```

As can be seen from the data most of the variables are skewed and some are also bimodal in shape. We might have to resort to some transformations like log transform, or other power transformation later on.

# Density plots of each variable

Let us also look at the density plots to get a more smoother look at the distributions.

```{r}

par(mfrow = c(3,4))

for(i in 5:15){
  
  plot(density(batDataset[,i]),main = names(batDataset)[i])
}

```

The density plots gives a more smoother plot. Another transformation which we can carry out is on the scaling of the variables. The values of variables have multiple scales. This can be re-scaled later on to get better results.

# Box and Whisker plots

Let us look at another perspective for the distribution of data. Let us look at the Box and Whisker plot for the dataset.

```{r}

par(mfrow = c(3,4))

for(i in 5:15){
  
  boxplot(batDataset[,i],main = names(batDataset)[i])
  
  
}
```
As seen from the box and whisker plots, many squished plots indicate the presence of extreme skewness in the data. Also long tails in plots indicate presence of outliers ??

Another intuition which can be taken away is the possibility of including a new variable related to Conductance Standard deviation like the voltage standard deviation.

# Multimodal Data Visualisations

Let us look at the interaction between the attributes. We will use a scatter plot matrix of the attributes coloured by the class values. 

```{r}

jittered_x <- sapply(batDataset[,5:15],jitter)

pairs(jittered_x,names(batDataset[,5:15]),col = batDataset$label)

```
# JMJPFU
# 6-Jan-2017

# Barplots

From the above jittered plot we can see that the wanted classes are in minority and at specific points accross some variables.


```{r}
par(mfrow = c(3,4))

for(i in 5:15){
  
  barplot(table(batDataset$label,batDataset[,i]),main = names(batDataset)[i])
  
  legend.text=unique(batDataset$label)
}
```
The above barplot is not that useful as expected to be. 

# Step 4 : Evaluation of Algorithms

In this step we will spot check some classes of algorithms to determine which algorithm will suite best this situation. The types of algorithms which we will test are the following

1. Linear Algorithms : Logistic Regression, Linear Discriminate Analysis(LDA) and Regularized Logistic Regression ( GLMNET)

2. Nonlinear Algorithms : K-Nearest Neighbours(KNN), CART, Naive Bayes & SVM

3. Ensemble Methods : Bagging : Bagged CART ( BAG), Randome Forest ( RF)

4. Ensemble Methods - Boosting : Stochastic Gradient Boosting(GBM) and C5.0 


## Training Setup

Let us go for a 10-fold cross validation with 3 repeats . For simplicity we will use Accuracy and Kappa metrics. We can also at a later stage go with Area Under ROC curve ( AUC ) and also look at sensitivity and specificity

### 4.a : Defining the Train controls

```{r}
trainCont <- trainControl(method = "repeatedcv", number = 10,repeats = 3)

metric <- "Accuracy"
```

### 4.b : Checking out the various algorithms

We will set seed so that all the algorithms train on the same split of data so that comparison is effective.

```{r}

# Logistic Regression - With glm - Fitted with error

set.seed(7)

#fit.glm <- train(label~.,data=batDataset[,5:16],method = "glm",metric = metric, trControl = trainCont)

# Linear Discriminate Analysis ( LDA ) - Fitted

#fit.lda <- train(label~.,data=batDataset[,5:16],method = "lda",metric = metric, trControl = trainCont)

# Regularized Logistic Regression ( GLMNET) : Didn't fit

# fit.glmnet <- train(label~.,data=batDataset[,5:16],method="glmnet",metric=metric, trControl = trainCont)

# KNN : Fitted

#fit.knn <- train(label~.,data=batDataset[,5:16],metric=metric,method = "knn", trControl = trainCont)

# CART : Fitted

# fit.cart <- train(label~.,data=batDataset[,5:16],method="rpart",metric=metric,trControl=trainCont)

# Naive Bayes : Fitted with lots of error

#fit.nb <- train(label~.,data=batDataset[,5:16],method = "nb",metric=metric, trControl = trainCont)

# Support Vector Machines : Fitted 

#fit.svm <- train(label~.,data=batDataset[,5:16],method="svmRadial",metric=metric,trControl=trainCont)



```

### Notes and Errors

1. A model was extracted with error. Errors point to the fact that glm works only for binary classification problems. Need to look for the prediction part
2. lda did work and a model was fitted. However there were some errors which cropped up related to collinearity
3. glmnet did not work. The error said that some of the classes had less than 8 observations. Need to try this method after the minority re-sampling methods are tried.
4. KNN worked. There werent any significant errors for this method.
5. Cart Worked. There werent many significant errors for this method.
6. Naive Bayes. This method also worked. However there were lots of errors which cropped up. From the look of it most of the errors were related to less variance in values for particular classes. Might be able to get rid of this by simulating values in the minority class sampling methods.
7. SVM - SVM fitted with not much error

### 4.c - Comparison of algorithms 

Once the initial set of algorithms are done, let us just compare these algorithms to find the better one among them.

```{r}

results <- resamples(list(LDA = fit.lda,KNN = fit.knn,CART = fit.cart,NB = fit.nb, SVM = fit.svm))

summary(results)

dotplot(results)
```

Looking at the results, the two best algorithms which fit well for the model are SVM and KNN. However a word of caution for the model is, the accuracy is high because of the fact that the majority class has brute majority over the minority class. So what we need to keenly check out is the sensitivity of the minority classes.

Now that we have the baseline models, let us try tuning the two best models , KNN & SVM

### 4.d . Tuning the parameters

Tuning KNN model

Let us use a ROC as the metric this time. ( see error note)

Metric of Accuracy is used


```{r}

trainControl <- trainControl(method = "repeatedcv",number = 10, repeats = 3,classProbs = FALSE)

metric <- "Accuracy" #

set.seed(7)

grid = expand.grid(.k=seq(1,20,by=1))

fit.knnTune <- train(label~.,data=batDataset[,5:16],method="knn",metric=metric,trControl=trainControl,tuneGrid=grid)

print(fit.knnTune)

plot(fit.knnTune)


```

Tuning SVM model

```{r}

trainControl <- trainControl(method = "repeatedcv", number = 10 , repeats = 3,classProbs = FALSE)

metric = "Accuracy"

set.seed(7)

grid <- expand.grid(.sigma=c(0.025,0.5,0.1,0.15),.C=seq(1,10,by=1))

fit.svmTune <- train(label~.,data=batDataset[,5:16],method="svmRadial",metric=metric,trControl=trainControl,tuneGrid=grid)

print(fit.svmTune)

plot(fit.svmTune)



```


# Errors & Notes

1. For the metric ROC, it is required to put classProbs = TRUE
2. There was another error which cropped when the metric ROC was used. The name of the label with underscore in it created problems. The class names have to be re-worked . However for the time being, let us change the metric from ROC and try Accuracy. Also the classProbs have to be kept as fALSE

# JMJPFU
## 9-Jan-2017

Today we will start the prediction process for the benchmark model. We will try modelling with both the KNN and SVM models. In addition we will also try out other models which were used for the training purpose to see how the results fare.

# 5 : Predicting the baseline model

## 5. a Prediction with the KNN model.

In the process of predicting using the KNN model in caret, we do not have to store a model. We have to take the training data and also the validation data and then use the parameters we got after the tunining process and do the prediction. For a check on the model accurcy let us also include the minority cases along with the validation data set.

## Preparing the data for prediction

```{r}

# Validation set preperation by adding minority classes from the training set

minorityData <- batDataset[batDataset$label != "Normal_Period",]

validationSet <- rbind(validationSet,minorityData) # Mising this data with the validationSet


```

## Prediction on the validation set

```{r}

set.seed(7)

validationSet$predict1 <- knn3Train(batDataset[,5:15],validationSet[,5:15],batDataset$label,k=8,prob = FALSE)

confusionMatrix(validationSet$predict1,validationSet$label)
```

Let us look at a table of prediction

```{r}
table(validationSet$predict1)
```

# Notes on prediction

1. When predicting with knn3Train() method, the traning set should not contain the labels. The labels should be represented seperately in the prediction part.


As seen from the prediction, the sensitivity of the other minority classes are zero. All the cases were tagged as normal_period. This result was what was expected. We will try to improve this through various methods like 
1. Transformations
2. Minority balancing

Let us in the mean time look up on some of the Confusion Matrix matrices definition.

## 1 : Prevelence

 Number of instances of a classs / total number of records 
 
 = 3/2083 for the minority classes ( 0.00144)
 
 = 2059/2083 for the majority class ( 0.98847)
 
## 2 : Sensitivity

= Number of instances of a class correctly predicted / Total number of instances of that class present

= 0/ 3 ( for minority classes)
= 2059/2059 ( for majority class)

## 3 : Specificity

Specificity is similar to the Sensitivity in that it looks for the same metric in the negative class.

= Number of instances of the -ve class correctly predicted / Total number of instances of the -ve class

## 4 : Positive Predictive Value

It is the proportion of the correctly predicted +ve values to the total number of positive values

= Correcly predicted +ve class / Total number of +ve class predicted

 = 2059/2059 ( for majority class)
 
 = 0/0 ( for minority class)

## 5 : Negative Predictive Value

Similar metric for negative class.

 = 2080/2083 ( for each minority class)
 
 = 0/0 ( majority class)


## Prediction using SVM

In the next pass we will try the prediction exercise using SVM and observe the results

# Tomorrow

1. Need to try prediction method with SVM as per the method in the caret website

# JMJPFU
### 10-Jan-2017

Let us first try the SVM with the native e1071 library and check how it performs. After which we will use it in conjunction with the caret package

```{r}
library(e1071)
```

Doing the training with the parameters defined earlier. Let us first do the training with a radial kernel with the earlier defined sigma and cost parameters

## SVM - Model 1 : Radial Kernel, C = 3 , Gamma = 0.025

```{r}

svmRadfit <- svm(label~.,data=batDataset[,5:16],kernel="radial",cost=3,gamma=0.025)

summary(svmRadfit)

table(svmRadfit$fitted,batDataset$label)

```

Looking at the fitted data, it can be seen that at least with the training data the SVM model looks much better than the KNN model earlier tried out. However the proof of the pudding will be when this is tried out on the validation data set.

Let us now do the prediction with the validation set

```{r}

validationSet$predict_SVM <- predict(svmRadfit,newdata = validationSet[,5:15])

table(validationSet$predict_SVM,validationSet$label)

```
For the validation set also, the other variables fared better than the KNN. One reason the other classes fared better is because of the reason that the other variables data is a copy paste from the training set. So it is bound to work well.

We will try the same with the testing dataset which we created earlier. 

### Creating the data set

```{r}

batTestSet <- normalCon %>% filter(label == "To_test")

batTestSet <- batTestSet %>% select(Battery,Counter,Bench1,Bench2,featDischarge,conSlope,dodMin,Dod50,Dod85,Dodtop,conDrop,contop,con80,contop,voltSD)

```

Predicting the model on the new test set

```{r}

batTestSet$predicSVMRad <- predict(svmRadfit,newdata = batTestSet[,5:15])

table(batTestSet$predicSVMRad)

```

Now that some prediction has been made, let us look at the batteries where the failure prediction was made and check its variables through visualisation

```{r}
failPredBats <- batTestSet %>% filter(predicSVMRad == "3_Period_before_Failure") %>% select(Battery)

failPredBatdat <- batTestSet %>% filter(Battery == failPredBats$Battery)

failPredBatdat2 <- batDataset %>% filter(Battery == failPredBats$Battery)

# Second data also

failPredBats1 <- batTestSet %>% filter(predicSVMRad == "4_Period_before_Failure") %>% select(Battery)

failPredBatdat3 <- batTestSet %>% filter(Battery == failPredBats1$Battery[2])

failPredBatdat4 <- batDataset %>% filter(Battery == failPredBats1$Battery[1])

# Let us take both batteries and then go for visualisation

failPredBats <- unique(rbind(failPredBats,failPredBats1))

```

Let us do some visualisation for these sets of batteries

```{r}
library(ggplot2)

Temp_bat_consol <- bat_newfeat5 %>% filter(Battery %in% failPredBats$Battery) %>% filter(measure %in% c("Conductance","DOD","Voltage")) # Taking the requuired data # ,"DOD","Voltage"


q5 <- ggplot(data=Temp_bat_consol,aes(as.factor(Date1),Variable,color=measure)) + geom_point() + facet_grid(measure~Battery,scales = "free") # ,margins =TRUE can be included if we want everything together in one graph

q5 + theme(axis.text.x=element_text(angle=70,hjust=1))

```

The visualisation indicated some problematic batteries. Let us also look at the periods when the prediction was done and visualise data before that period.

```{r}

failPredBats <- batTestSet %>% filter(predicSVMRad == "3_Period_before_Failure") %>% select(Battery,Bench1,Bench2)

failPredBatdat <- batTestSet %>% filter(Battery == failPredBats$Battery)

failPredBatdat2 <- batDataset %>% filter(Battery == failPredBats$Battery)

# Second data also

failPredBats1 <- batTestSet %>% filter(predicSVMRad == "4_Period_before_Failure") %>% select(Battery,Bench1,Bench2)

failPredBatdat3 <- batTestSet %>% filter(Battery == failPredBats1$Battery[2])

failPredBatdat4 <- batDataset %>% filter(Battery == failPredBats1$Battery[1])

# Let us take both batteries and then go for visualisation

failPredBats <- unique(rbind(failPredBats,failPredBats1))


```

Visualising the data 

```{r}
library(ggplot2)

Temp_bat_consol <- bat_newfeat5 %>% filter(Battery %in% failPredBats$Battery[1]) %>% filter(measure %in% c("Conductance","DOD","Voltage")) %>% filter(Date <= "2015-05-02") # Taking the requuired data # ,"DOD","Voltage"


q5 <- ggplot(data=Temp_bat_consol,aes(as.factor(Date1),Variable,color=measure)) + geom_point() + facet_grid(measure~Battery,scales = "free") # ,margins =TRUE can be included if we want everything together in one graph

q5 + theme(axis.text.x=element_text(angle=70,hjust=1))

```


So taking a look at the periods in which the batteries were showing some weakness, the prediction has been correct so far.

### Thank you My Lord

# Tomorrow . Have to try the following
1. Try ROC curves as per ROCR package : ISLR page 365
2. TRY SVM with the following kernels : Polynomial, linear. For polynomial we also have to specify the degree of the polynomial.
3. TRY SVM with scaling ( Scale = TRUE)
4. Try SVM with the tuning method # ISLR page 361
5. Try out various options in Caret package

# JMJPFU
### 11-Jan-2017

Let us first try the SVM using the tuning method. 

## Tuning : Tuning can be done using the function tune():

Let us set the following paramaeters for tuning : 
1. Cost
2. Sigma
3. kernel

```{r}
set.seed(7)

tuneSvm <- tune(svm,label~.,data=batDataset[,5:16],ranges = list(gamma=c(0.025,0.5,0.1,0.15,1),cost=seq(1,10,by=1),kernel=c("radial","linear","polynomial")))

summary(tuneSvm)

```

So from the tuning of the variables the best performance is with the following parameters

- best parameters:
 gamma cost kernel
 0.025    1 radial

- best performance: 0.002905276 

Once tuning is done and the best possible parameters are identified, let us predict with the best available parameters.

### Predicting with best model which was output from Tuning

```{r}
bestmod = tuneSvm$best.model

summary(bestmod)

# Predicting with the best model

batTestSet$predicSVMTune <- predict(bestmod,newdata = batTestSet[,5:15])

table(batTestSet$predicSVMTune)

# Predicting again with the old model

batTestSet$predicSVMRad2 <- predict(svmRadfit,newdata = batTestSet[,5:15])

table(batTestSet$predicSVMRad2)

```
The new prediction predicted all the cases as Normal_period. I think the prediction with the first model ( svmRadfit) was the better of the lot.

### Using the ROCR library for ROC curves

Let us not attempt and use the ROCR library

```{r}
library(ROCR)

pred <- validationSet$predict_SVM # Converting to a vector

truth <- validationSet$label # Converting to a vector

predObject <- prediction(pred,truth)

perf <- performance(predObject,"tpr","fpr")

plot(perf)

```

### Errors and Notes
1. An error message cropped up related to the format of the data. It seems the vector of predictions is required in numerical format. Additional bit of massaging has to be done in the data to get to that level. The additional massaging to the data is as below.

We need to get the fitted values for the class labels if the ROCR library is to work. The fitted labels can be got as below


```{r}

svmFitted <- svm(label~.,data=batDataset[,5:16],kernel="radial",cost=3,gamma=0.025,decision.values=T)

fittedSvm <- attributes(predict(svmFitted,batDataset[,5:16],decision.values = TRUE))$decision.values

predObject <- prediction(fittedSvm,batDataset$label)

perf <- performance(predObject,"tpr","fpr")

plot(perf)


```

### Tomorrow

1. Proceed further with other methods of SVM from Caret Package.
2. Proceed with training with other methods like LDA, GLMNET,XGBOOST etc

## JMJPFU
### 18-Jan-2017

Let us try to fine tune this model with transformations of the initial Data set and create a new modelling process. These are the things which we will do

1. Transform the variables by using various transformation methods.
2. Create the new model with the transformed variables with SVM
3. Try out prediction from the new model.
4. Look at different type of metrics for the model
5. Try the whole steps for various algorithms - XGBOOST, RF, LDA, GLMNET, LR etc

## JMJPFU
### 19-jan-2017

From today we will resort to preproccing of data and transforming the data and try out the modelling part

## Step 1: Get the Training and Validation Dataset and plotting the variables

```{r}
par(mfrow=c(4,6))

for(i in 5:15){
  
  hist(batDataset[,i],main=names(batDataset)[i])
  plot(density(batDataset[,i]),main=names(batDataset)[i])
}

```

So as seen from the plots, of the variables. Most of the data sets need to be transformed in one way or the other. So lets go variable by variable and apply various transformations.

### Variable 1 : featDischarge
Skew Type : Right skewed and exponential
Data Values Range : All positive and non zero
Type of Transformation : Box cox transform, since all values are positive

### Variable 2 : dodMin
Skew Type : left skewed and exponential
Data Values Range : Positive
Type of Transformation : YeoJohnson

### Variable 3 : Dod50
Skew Type : Right skewed and exponential
Data Values Range : Positive
Type of Transformation : YeoJohnson
Scaling required : Yes as the ranges are from 0 to 50. So method "range" used to normalise data

### Variable 4 : contop & Con80
Skew Type : Right skewed and exponential
Data Values Range : Positive
Type of Transformation : YeoJohnson
Scaling required : Yes as the ranges are from 0 to 100. So method "range" used to normalise data

### Variable 5 : conDrop & conTop
Skew Type : left skewed and exponential
Data Values Range : Positive
Type of Transformation : YeoJohnson
Scaling required : Yes. ConDrop is between 0 and 1. ConTop most of the values are near 100 so maybe scaling can be done 

### Variable 6 : voltsd
Skew Type : Right skewed and exponential
Data Values Range : Positive
Type of Transformation : YeoJohnson
Scaling required : Yes. 

```{r}

# Creating a new data set for transformations
batDatTran <- batDataset

# featDischarge
range(batDatTran$voltSD)

summary(batDatTran$voltSD)

str(batDatTran$voltSD)



```

Applying the transformations

```{r}

# Applying the transformation


featTrans <- preProcess(batDatTran[,c(15,9)],method=c("range","YeoJohnson")) # "range" is to normalize the data

print(featTrans)


transformed <- predict(featTrans,batDatTran[,c(15,9)])

#batDatTran$Dod50 <- transformed$Dod50

batDatTran[,15] <- transformed[,1]

# Let us look at some visualisation

par(mfrow=c(2,2))

hist(batDatTran$voltSD,main=names(batDatTran$voltSD))
hist(batDataset$voltSD,main=names(batDatTran$voltSD))
plot(density(batDatTran$voltSD),main=names(batDatTran$voltSD))
plot(density(batDataset$voltSD),main=names(batDatTran$voltSD))


```

So as seen using either BoxCox or YeoJohnson transformations result in near normal kind of distributions

# Tomorrow
Continue with the transformation and then further modelling

### JMJPFU
### 20-Jan-2017

Continuing with the transfomation and then modelling.

Now that the transformation is complete let us visualise the transformed data once more 

```{r}

par(mfrow=c(4,6))

for(i in 5:15){
  
  hist(batDatTran[,i],main=names(batDatTran)[i])
  plot(density(batDatTran[,i]),main=names(batDatTran)[i])
}

```
Let us now start the modelling part and check the results. We have to carry out the spot checking process to find the best algorithm again with the transformed data and then do fine tuning to the best algorithm.

## Step 2 : Spot checking algorithms to find the best algorithm


In this step we will spot check some classes of algorithms to determine which algorithm will suite best this situation. The types of algorithms which we will test are the following

1. Linear Algorithms : Logistic Regression, Linear Discriminate Analysis(LDA) and Regularized Logistic Regression ( GLMNET)

2. Nonlinear Algorithms : K-Nearest Neighbours(KNN), CART, Naive Bayes & SVM

3. Ensemble Methods : Bagging : Bagged CART ( BAG), Randome Forest ( RF)

4. Ensemble Methods - Boosting : Stochastic Gradient Boosting(GBM) and C5.0 


## Training Setup

Let us go for a 10-fold cross validation with 3 repeats . For simplicity we will use Accuracy and Kappa metrics. We can also at a later stage go with Area Under ROC curve ( AUC ) and also look at sensitivity and specificity

### 2.a : Defining the Train controls

```{r}
trainCont <- trainControl(method = "repeatedcv", number = 10,repeats = 3,classProbs = TRUE)

metric <- "ROC"
```

### 2.b : Changing the names of the classes

This is required because the names of classes with "_" is generating error when the training is going on. So we will remove the underscores

```{r}

unique(batDatTran$label)

batLabels <- batDatTran %>% select(label)

batLabels$label <- as.character(batLabels$label) # First convert into charchter from factor. Only then replacement is possible

# Doing the substitution
batLabels[batLabels$label=="Normal_Period",1] <- paste("NormalPeriod")
batLabels[batLabels$label=="Failure_Point",1] <- paste("FailurePoint")
batLabels[batLabels$label=="1_Period_before_Failure",1] <- paste("PeriodbeforeFailure1")
batLabels[batLabels$label=="2_Period_before_Failure",1] <- paste("PeriodbeforeFailure2")
batLabels[batLabels$label=="3_Period_before_Failure",1] <- paste("PeriodbeforeFailure3")
batLabels[batLabels$label=="4_Period_before_Failure",1] <- paste("PeriodbeforeFailure4")
batLabels[batLabels$label=="5_Period_before_Failure",1] <- paste("PeriodbeforeFailure5")
batLabels[batLabels$label=="6_Period_before_Failure",1] <- paste("PeriodbeforeFailure6")
batLabels[batLabels$label=="7_Period_before_Failure",1] <- paste("PeriodbeforeFailure7")

# Comparing the results

table(batLabels$label)
table(batDatTran$label)

# Changing back to factors

batLabels$label <- as.factor(batLabels$label)
str(batLabels)

# Substituting the transformed dataset labels with the new ones

batLabels1 <- batDatTran %>% select(label)

batDatTran$label <- batLabels$label

str(batDatTran)

batLabels <- batLabels1

```







### 2.c : Checking out the various algorithms

We will set seed so that all the algorithms train on the same split of data so that comparison is effective.

```{r}


set.seed(7)

# Linear Discriminate Analysis ( LDA ) - Fitted

#fit.lda <- train(label~.,data=batDatTran[,5:16],method = "lda",metric = metric, trControl = trainCont)

# KNN : Fitted

#fit.knn <- train(label~.,data=batDatTran[,5:16],metric=metric,method = "knn", trControl = trainCont)

# CART : Fitted

# fit.cart <- train(label~.,data=batDatTran[,5:16],method="rpart",metric=metric,trControl=trainCont)

# Naive Bayes : Fitted with lots of error

#fit.nb <- train(label~.,data=batDatTran[,5:16],method = "nb",metric=metric, trControl = trainCont)

# Support Vector Machines : Fitted 

#fit.svm <- train(label~.,data=batDatTran[,5:16],method="svmRadial",metric=metric,trControl=trainCont)



```

### Notes and Errors

1. When ROC is used as a metric, class probabilities is required in trainControl function
2. The class names are not in the proper format. Will have to change the names of the classes
3. Even after the substitution it did not work because the names were still wrong. The method of putting a number before a name is wrong. We can try by putting a number after the name. Note : This worked
4. Metric ROC did not work. Accuracy was used instead for LDA

### JMJPFU
### 23-Jan-2017

Today we will continue with the modelling part with the transformed dataset.First we will evaluate all the algorithms and find which is the best among them for the transformed dataset.

### 2.d : Checking various algorithms

Let us now check the results of the various algorithms.

```{r}
results <- resamples(list(LDA = fit.lda,KNN = fit.knn,CART = fit.cart,NB = fit.nb,SVM = fit.svm))

summary(results)

dotplot(results)

```

As seen from the results, SVM seems to be the best algorithm again. First let us tune the SVM algorithm to find the most optimal parameters. Then et us try our prediction with the validation set and also the new data set and do a grid search to find the best parameters for tuning the SVM model.

After doing the above we will also do the minority re-sampling process and then do a modelling based on that. 

### 2.e : Checking various algorithms

Fine tuning the SVM algorithm.

```{r}

trainCon <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

metric = "Accuracy"

set.seed(7)

grid <- expand.grid(.sigma = c(0.010,0.025,0.5,0.1,0.15), .C=seq(1,5,by=.5))

fit.svmTune1 <- train(label~.,data=batDatTran[,5:16],method="svmRadial",metric=metric,trControl=trainCon,tuneGrid=grid)

print(fit.svmTune1)

plot(fit.svmTune1)

```

So as seen from the tuning the best set of parameters within this lot are 

sigma = 0.5
Cost = 1

Let us try these parameters and look at training the model with these parameters

Let us use the library e1071 for training the SVM model

```{r}
library(e1071)
```

### 2.f : Modelling with the optimal parameters and native SVM model

SVM Parameters : sigma = 0.5, Cost = 1

```{r}

svmFit1 <- svm(label~., data = batDatTran[,5:16],gamma = 0.5,cost = 1,kernel = "radial")

summary(svmFit1)

table(svmFit1$fitted,batDatTran$label)

```

### Step 3 : Prediction on the testing data set


```{r}
batTestTran <- batTestSet
```


```{r}
# Transforming the testing set

featTrans <- preProcess(batTestTran[,c(5,7,10,11,13,14,15)],method = c("range","YeoJohnson"))

transformed <- predict(featTrans,batTestTran[,c(5,7,10,11,13,14,15)])

batTestTran[,c(5,7,10,11,13,14,15)] <- transformed

# Transforming the validation set

featTrans <- preProcess(validationSet[,c(5,7,10,11,13,14,15)],method = c("range","YeoJohnson"))

transformed <- predict(featTrans,validationSet[,c(5,7,10,11,13,14,15)])

validationSet[,c(5,7,10,11,13,14,15)] <- transformed

```

### 3.b : Predicting on the transformed dataset

```{r}
batTestTran$predicSVMTran1 <- predict(svmFit1,newdata=batTestTran[,5:15])

table(batTestTran$predicSVMTran1)

# Trying the above on the validation set

validationSet$predicSVMTran1 <- predict(svmFit1,newdata=validationSet[,5:15])

table(validationSet$predicSVMTran1)

# Let us try the old model also

batTestTran$predicSVMTran2 <- predict(svmRadfit,newdata=batTestTran[,5:15])

table(batTestTran$predicSVMTran2)


```
The prediction on the transformed data set is all one dimensional. Let us try prediction without transforming the data and see how it fares.

```{r}
batTestSet$predicSVMRad3 <- predict(svmFit1,newdata=batTestSet[,5:15])

table(batTestSet$predicSVMRad3)

```

So as seen the modelling on the transformed data set did not yield good results with both the test set and the validation sets.

### JMJPFU
### 24-Jan-2017

Today we will try the minority sampling methods of Caret package and do the modelling with both transformation of the variables and without transformation of variables. Will also try the ensemble methods.

Let us use the SMOTE method first

### Installing the required library

```{r}
library(DMwR)

```

### Taking the data set and creating the minority classes

```{r}
batMinSet <- batDataset[,5:16]

batMinSet$label <- batDatTran$label

batMinSet <- SMOTE(label~.,batMinSet,perc.over = 1000)

table(batMinSet$label)

unique(batMinSet$label)

```


### Errors and notes

1. First command : batMinSet <- SMOTE(label~.,batMinSet,perc.over = 1000,perc.under=100) : This created a data set with only with two classes. The majority classes was downgraded to about 30. All other classes was 0, 1_period was 33. Need to try without the per.under


So all the different tries resulted in only two classes which get sampled. So one method to try is to take two classes each and then do the SMOTe algorithm on the two and then keep aggreagating the data. 

```{r}
datSamp1 <- batMinSet %>% filter(label %in% c("NormalPeriod","PeriodbeforeFailure7"))

datSamp1$label <- as.character(datSamp1$label) # To remove other levels

datSamp1$label <- as.factor(datSamp1$label) # To refactor the labels

datMin <- SMOTE(label~.,datSamp1,perc.over = 20000,perc.under = 100)

datMin <- datMin[complete.cases(datMin),]

table(datMin$label)

```

Consolidating all the SMOTe data sets into one

```{r}
batMinFin <- batMinSet %>% filter(label == "NormalPeriod")

```

```{r}
datMin <- datMin %>% filter(label == "PeriodbeforeFailure7" ) # Taking only the Failure Point classes

batMinFin <- rbind(batMinFin,datMin)

table(batMinFin$label)
```
### JMJPFU
### 25-Jan-2017

Continue with training with the new data set and then try validating with the current data set to check how it fares.

We can try transformation of the variable and without transformation.

### Step 1 : Creating a Training set and a validation set with the SMOTe data set

```{r}

set.seed(7)

# Creating the validation Index

validationIndex <- createDataPartition(batMinFin$label,p=0.80,list = FALSE)

# Creating the validation set

validationSet1 <- batMinFin[-validationIndex,]

# Creating the Dataset

batDataset1 <- batMinFin[validationIndex,]

table(validationSet1$label)

table(batDataset1$label)


```

### Step 2 : Doing a spot check with various algorithms on the new SMOTe data set


#### Step 2a : Defining the train control and metrics


```{r}
trControl <- trainControl(method="repeatedcv",number = 10, repeats = 3,classProbs = TRUE)

metric <- "ROC"
```


### Step 2b : Checking whether the labels are factors or not and then converting it into factors

```{r}

str(batDataset1$label)

str(validationSet1$label)

```

Both are factors

### Step 2c : Doing the spot check with various algorithms

```{r}
set.seed(7)

# Logistic Regression with glm : Does not fit for multiclass problems

# glm.fit1 <- train(label~.,data=batDataset1,method="glm",metric=metric,trControl = trControl)

# Boosted Generalised LInear model : Did nto have the required packages. Will try out later

# BGLM.fit1 <- train(label~.,data=batDataset1,method="glmboost",metric=metric,trControl=trControl)

# Linear Discriminant Analysis : Fitted by with metric as accuracy

# lda.fit1 <- train(label~.,data=batDataset1,method = "lda",metric= metric,trControl=trControl)

# Generalised linear models : Fitted with lots of errors

#glm.fit1 <- train(label~.,data=batDataset1,method="glmnet",metric=metric,trControl=trControl)

# KNN : Fitted with no errors

# knn.fit1 <- train(label~.,data=batDataset1,method="knn",metric=metric,trControl=trControl)

# CART : Fitted with no error

# cart.fit1 <- train(label~.,data=batDataset1,method="rpart",metric=metric,trControl=trControl)

# Naive Bayes : Fitted with lots of errors

# nb.fit1 <- train(label~.,data=batDataset1,method="nb",metric=metric,trControl=trControl)

# SVM : Fitted without much error.

# svm.fit1 <- train(label~.,data=batDataset1,method="svmRadial",metric=metric,trControl=trControl)


```


## Errors and notes

1. Glm : Did not fit for multi class problems.
2. LDA : Fitted with metric as Accuracy
3. GLMnet : Fitted with lots of errors
4. None of the methods used ROC. It seems ROC as a metric can only be used for 2 class problems.

# Tomorrow

1. Model with the ensemble methods also
2. Check how the model performs for both ensemble and other set of models
3. Try prediction also



