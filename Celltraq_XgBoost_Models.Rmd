---
title:"JMJPFU-XGBoost Model"
output: html_notebook
---

### JMJPFU 
### 2-Feb-2017 : 

This is a sample notebook for XGBoost model. 

### Step1 : Setting the library functions

```{r}
library(xgboost)
library(Matrix)
library(data.table)
library(Rtsne)
library(caret)
library(dplyr)
library(ggplot2)
```


### Step 2 : Preparing the labels using one hot encoding

XGboost works only with numerical data. Therefore the label data has to be converted using one-hot encoding.

```{r}
xTrainohe <- xTrain
oheLabel <- sparse.model.matrix(label~.-1,data=xTrainohe)
head(oheLabel)
```
Now converting the label data into numeric form.

```{r}
xLabel <- xTrain[,"label"]

levels(xLabel)

num.class <- length(levels(xLabel))

levels(xLabel) <- 1:num.class

head(xLabel,50)
  
  
```

Let us ignore the earlier created sparse matrix and create a new train matrix without including the label in it.

```{r}

xTrainohe <- xTrainohe[,1:11] # Exclusing the labels vector
```

Let us now try out a tSNE ( t-Distributed Stochastic Neighbour Embedding) visualisation 

Reference Link : https://rpubs.com/flyingdisc/practical-machine-learning-xgboost

Reference Link : https://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/

Reference Link : http://jamesmarquezportfolio.com/ # This is really good one


```{r}
tsne <- Rtsne(as.matrix(xTrainohe),check_duplicates = FALSE,pca = TRUE,perplexity = 30,theta = 0.5,dims=2)

summary(tsne)
```

Visualising the same

```{r}
library(ggplot2)

embedding = as.data.frame(tsne$Y)
embedding$Class = xTrain[,"label"]

g = ggplot(embedding,aes(x=V1,y=V2,color=Class)) + geom_point(size=1.25)+ guides(colour=guide_legend(override.aes = list(size=6))) + xlab("") + ylab("") + ggtitle("t-SNE 2D Embedding of 'Label' Outcome") + theme_light(base_size = 20) + theme(axis.text.x=element_blank(),axis.text.y=element_blank())

print(g)
```

Looking at the 2d embedding we can see some clusters being formed. 

### Generating a random number between 1 & 736

```{r}

rand <- sample(1:736,1)

#write.csv(rand,"BibleRead.csv")

sample(1:736,1)

```

### JMJPFU
### 3-Feb-2017


1. Continue with the Xgboost model example and wrap it up
2. Start with new methods and models

```{r}
# Convert data into a matrix
xTrainMatrix = as.matrix(xTrainohe)
mode(xTrainMatrix) = "numeric"

# xgboost takes multi-labels in [0,numof classes]
y = as.matrix(as.integer(xLabel)-1)
```

### Step 3.a : Setting the xGBoost Parameters

```{r}
# xgboost parameters

param <- list("objective"= "multi:softprob", # multiclass classification
              "num_class" = num.class, # Number of classes
              "eval_metric" = "merror", # Evaluation metric
              "nthread" = 8, # number of threads to be used
              "max_depth" = 16, # maximum depth of tree
              "eta" = 0.3, # step size shrinkage
              "gamma" = 0, # minimum loss reduction
              "subsample" = 1, # part of data instances to grow tree
              "colsample_bytree" = 1, # subsample ratio of columns when constructing each tree
              "min_child_weight" = 12 # minimum sum of instance weight needed in a child
              )
```

### Step 3.b : 10-fold cross validation

```{r}
set.seed(1234)

nround.cv = 200

system.time(bst.cv <- xgb.cv(param=param, data=xTrainMatrix,label=y,nfold = 10,nrounds = nround.cv,prediction = TRUE,verbose = FALSE))
```

Checking the cv scores

```{r}
head(bst.cv,10)
```
Let us look at the cross validation score and find the index with the minimum multiclass error rate

```{r}
# index of minimum merror

min.merror.idx = which.min(bst.cv$evaluation_log$test_merror_mean)

min.merror.idx

bst.cv$evaluation_log[min.merror.idx]
```

The minimum error rate is for iteration 157 as shown above

### Step 3.c : Confusion matrix

```{r}
pred.cv <- matrix(bst.cv$pred,nrow=length(bst.cv$pred)/num.class,ncol=num.class)

pred.cv <- max.col(pred.cv,"last") # Taking the max accross the columns which is the probability of each class

# Confusion matrix


library(caret)
confusionMatrix(factor(y+1),factor(pred.cv))
```

Thank you Lord for the progress so far in the xgboost model.

### JMJPFU
### 6-Feb-2017

We will pursue with more experiments with xgboost later on. Let us stop the methods for now and then continue with another method of prediction with another algorithm.

# JMJPFU
### 20-Mar-2017

We will embark on a detailed data science journey document with XGboost. To embark on the journey we will first do some experiments with some methods and then get on to the documentation phase.

### Experiments 1 : Zero variance

```{r}

# Near zero variance experiments

zero.var <- nearZeroVar(xTrain[,1:11],saveMetrics = TRUE)

zero.var


```

So as seen "Dod50" and "con50" variables have near zero variance and therefore can be excluded from the model after some after thought.

### Experiments 2 : Plot of relationship of features with Outcome

# JMJPFU
#### 21-Mar-2017:

Continuing with experiment 2 to analyse the relationship of features with outcome.

```{r}
featurePlot(xTrain[,1:11],factor(y),"strip")


```

As we can see, the data has different distribution for each of the feature. However, we need to do scaling of the variables and see how they look like with the scaled features.


### Data Science Safari : ##################

Now we will start the data science safari and explore the inner meaning of distributions processes and its connections with the model. The below is the path we will tread

I . Data Cleaning
II. Statistical Exploration
  1. Look at the distribution of the raw variables
  2. Understand the process which generated the data
  3. Get an intuition on the probability model which might have generated the data.
  4. Get some descriptive statistics ( Mean, Median, Mode, SD) for each of the variable
  5. Get histograms, density plots etc for each variable.
  6. Get to know the distributions of the variables and compare it with the intuitions we build about the process which generated data
  7. Look at any linearity assumptions of data
  8. Look at the plots of each variable with respect to the response variables
  9. Look at the correlation plots with respect to the variables
  10. Look at the class distribution of the classes. Get clues on any minority sampling techniques to be used
  11. Get intuitions on features that needs to be developed in the context of the business problem.
  
III. Feature Engineering
Iv . Modelling
V. Deployment.

## Safari : Statistical Exploration

Let us now look at the distribution of the raw variables of multiple samples

The process to do this is as follows
1. Filter out the data set of a battery
2. Create multiple histogram for each variable
3. Study the distributions of each variable accross multiple samples.
4. Write down the intuitions

```{r}
# Filtering out the battery

expBat <- unique(batConsolidated$Battery)[1]
expBat

# Now filtering out the relevant data for this battery

batData <- batConsolidated %>% filter(Battery == expBat)

# Now filtering out the relevant variable data

voltData <- batData %>% filter(measure == "Voltage")
curData <- batData %>% filter(measure == "Current")
conData <- batData %>% filter(measure == "Conductance")
voltempData <- batData %>% filter(measure == "Volttemp")
charData <- batData %>% filter(measure == "Charge" )
disData <- batData %>% filter(measure == "Discharge")
dodData <- batData %>% filter(measure == "DOD")

# Now to viaulise the data

par(mfrow = c(4,2))
a = hist(voltData$Variable, main = "Voltage");plot(a);abline(v=mean(voltData$Variable),col="red")
b= hist(curData$Variable, main = "Current");plot(b);abline(v=mean(curData$Variable),col="red")
c= hist(conData$Variable, main = "Conductance");plot(c);abline(v=mean(conData$Variable),col="red")
d = hist(voltempData$Variable, main = "NormVoltage");plot(d);abline(v=mean(voltempData$Variable),col="red")
e = hist(charData$Variable, main = "Charge" );plot(e);abline(v=mean(charData$Variable),col="red")
f = hist(disData$Variable, main = "Discharge");plot(f);abline(v=mean(disData$Variable),col="red")
g = hist(dodData$Variable, main = "DOD");plot(g);abline(v=mean(dodData$Variable),col="red")

```

```{r}
unique(batData$measure)
```

# JMJPFU
### 22-Mar-2017

Today we will try to approach the exploratory stage from a Bayesian stand point and then try to learn and get some assumptions of the model. Will try to extrapolate this assumptions and do some statistical learning from the data.

# Assumptions to the model : Naive Bayes Assumption

The first assumptions we are making to the model is that each of the varaible are independent of the other ( Which is obviously an erraneous assumptions. However as a first pass for modelling and learning we will go by this assumption.).


### Step 1 : Look at distributions of each variable

Since each of the variables are numeric, let us take some sample variables and get some of the essential stats behind each of them like

1. Mean of the distribution
2. Standard deviation

```{r}

# Filtering each battery
expBat <- unique(batConsolidated$Battery)[1]
expBat
# Filtering some relevant data and getting their descriptive stats
statData <- batConsolidated %>% filter(Battery == expBat) %>% group_by(measure) %>% summarise(Mean = mean(Variable),SD = sd(Variable))

# Transposing the data
temp <- t(statData)

# Creating an empy data set

statDat <- data.frame(matrix(nrow=0,ncol=3))

names(statDat) <- c("Battery","Mean","SD")

```

The above can be come too complicated at this point of time. For the time being, let us just take only one variable

```{r}

for(i in 1:length(unique(batConsolidated$Battery))){
# Filtering each battery
expBat <- unique(batConsolidated$Battery)[i]
#expBat

# Filtering the relevant data and gathering descriptive stats

staData <- batConsolidated %>% filter(Battery == expBat & measure == "Voltage")%>% summarise(Battery = unique(Battery),Mean = mean(Variable),SD = sd(Variable))

# Consolidating the data

statDat <- rbind(statDat,staData)

}

## Will divide the batteries into two sets. 

statData2 <- statDat %>% filter(Mean < 10)
statData12 <- statDat %>% filter(Mean > 10)

```

Now that we have the means and standard deviation let us plot the means in a histogram and take the mean value of the means

```{r}
par(mfrow = c(1,2))

plot(density(statData2$Mean),main = "Mean 2")
abline(v = mean(statData2$Mean),col="red")

plot(density(statData12$Mean),main = "Mean 12")
abline(v = mean(statData12$Mean),col="red")

```

Now that we have taken the histogram of the collection of batteries, let us look at each of the values and look of each of the battery and compare with the group mean and then get some inferences

# Calculating the likelihood of the values
In addition for each battery let us also look at the likely hood and then plot the likelihood


```{r}
# Filtering one battery at a time
i = 1
expBat <- unique(batConsolidated$Battery)[i]

# Taking the relevant data
con1 = sqrt(2*3.14)
con2 = 1 /(con1 * statDat[i,3] )
con4 = 1/(2*statDat[i,3]^2)

statData1 <- batConsolidated %>% filter(Battery == expBat & measure == "Voltage") %>% mutate(con3 = (Variable - statDat[i,2]),con5 = con3^2) %>% mutate(LPDF = (con2 * exp(-con5*con4) ) )

# Visualising the same

par(mfrow = c(1,2))

plot(density(statData1$Variable),main = "Voltage");abline(v=statDat[i,2],col="red")

plot(density(statData1$LPDF),main = "Likelihood");abline(v=statDat[i,2],col="red")

```

Forget about the likelihood for now. It dosent make sense right now

### Approach 2 : With the training data set

Let us take a different approach. Let us now go with the training data set and then apply bayes rule along with the distribution of the classes


### Step 1 : Finding the class probabilities

```{r}

# Class probabilities

classProb <- data.frame(prop.table(table(xTrain$label)))

```

# JMJPFU
### 23-Mar-2017

Today we will continue with the Bayesian Inference and do the inferential studies.

```{r}
head(xTrain)
```


The model is as drawn in the Bayesian Network model. Now we will have to populate the conditional values for this network. There are different thoughts on the data itself. As the xTrain model is a simulated data set, it might not present the right picture. However, what we have to do is work out the sample model with this data. Get some intuitions and then do improvements with the real data set. 

Let us try to bootstrap with the real data set and try to learn from the conditional probabilities.

Some of the learnings which could be applied

1. For continuous variables - We can work out the approximate distribution and assume the hyper parameters based on the 

# JMJPFU
### 24-Mar-2017 : Continuing with the Statistical Intuition of a data analysis.

### Step 1 : Start from the labels and looking at the Distributions of various classes

NormalPeriod         FailurePoint         PeriodbeforeFailure1 PeriodbeforeFailure2 PeriodbeforeFailure3 PeriodbeforeFailure4
PeriodbeforeFailure5 PeriodbeforeFailure6 PeriodbeforeFailure7

```{r}
## Normal - DODmin

classDistribution <- xTrain %>% filter(label=="NormalPeriod")

par(mfrow = c(3,4))

plot(density(classDistribution$dodMin),main = "dodMin")
plot(density(classDistribution$Dod50),main = "Dod50")
plot(density(classDistribution$Dod85),main = "Dod85")
plot(density(classDistribution$Dodtop),main = "Dodtop")
plot(density(classDistribution$conDrop),main = "conDrop")
plot(density(classDistribution$con50),main = "con50")
plot(density(classDistribution$con80),main = "con80")
plot(density(classDistribution$contop),main = "contop")
plot(density(classDistribution$voltSD),main = "voltSD")
plot(density(classDistribution$conSlope),main = "conSlope")
plot(density(classDistribution$featDischarge),main = "featDischarge")

```

