
# JMJPFU
# 27-Jan-2017
# Random Forest Implementation for Celltraq

### Reference Link

http://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/

Search keyword in Google : Random Forest tuning with Caret

First we will try Random Forest Models using the caret package methods. After that we will use Random Forest with the native methods.

## Random Forest Implementation with the Caret Package Methods


### Step A : Loading the required libraries

```{r}
library(plyr)
library(dplyr)
library(caret)
library(ggplot2)
```


### Step 1. Preparing and Preprocessing the training and validation sets

```{r}
set.seed(7)

# Preprocessing 

xTrain <- batDataset1

xVal <- validationSet1

preprocessParamsTr <- preProcess(xTrain,method = c("BoxCox"))

xTrain <- predict(preprocessParamsTr,xTrain)

preprocessParamsVl <- preProcess(xVal,method=c("BoxCox"))

xVal <- predict(preprocessParamsVl,xVal)



```

### Step 2. Tuning the Random Forest Model with traincontrol and grid expansion - Baseline

The parameters which we will tune are the following

1. mtry : Number of variables randomly sampled as candidates for each split
2. ntree : Number of trees to grow

For the baseline we will try the following

mtry = floor(sqrt(ncol(xTrain)))
ntree = 500



```{r}
# Setting the traincontrol parameters
trainConRf <- trainControl(method = "repeatedcv",number=10,repeats=3)

seed = 7

metric = "Accuracy"

set.seed(seed)

mtry = floor(sqrt(ncol(xTrain)))

# Setting the Grid Expand

gridRf <- expand.grid(.mtry=mtry)

rfDefault <- train(label~.,data=xTrain,method="rf",metric=metric,tuneGrid=gridRf,trControl=trainConRf)

print(rfDefault)



```
Let us now try the prediction of the default RF model

```{r}
xVal$rfDefault <- predict(rfDefault,newdata = xVal[,1:11])

confusionMatrix(xVal$label,xVal$rfDefault)
```
Just trying out on the batTestSet also

```{r}
batTestSet$rfDefault <- predict(rfDefault,newdata = batTestSet[,5:15])

table(batTestSet$rfDefault)
```

## The git repository is Quadrant4. The file is picked from the git repository Quadrant4

## JMJPFU
### 30-Jan-2017
1. Let us first check the results we got 
1. Continue with the tuninig and prediction of the RF model as per the ML mastery website

### Step 3 : Testing the results of prediction through visualisation.

```{r}
# Getting those batteries which are indicated as failed

failedBats <- unique(batTestSet %>% filter(rfDefault != "NormalPeriod") %>% select(Battery))

```

Visualising the batteries

```{r}

Temp_bat_consol <- bat_newfeat5 %>% filter(Battery %in% failedBats$Battery[12]) %>% filter(measure %in% c("Conductance","DOD","Voltage")) #  %>% filter(Date <= "2015-05-02") # Taking the requuired data # ,"DOD","Voltage"

q5 <- ggplot(data=Temp_bat_consol,aes(as.factor(Date1),Variable,color=measure)) + geom_point() + facet_grid(measure~Battery,scales = "free") # ,margins =TRUE can be included if we want everything together in one graph

q5 + theme(axis.text.x=element_text(angle=70,hjust=1))


batTestSet %>% filter(Battery %in% failedBats$Battery[12]) %>% select(rfDefault)



```


So as seen from the visualisations, the predictions were reasonably oK. Let us now continue with the other methods as per the website.

### Step 4.a : Tuning the parameter with Random search

Let us search for the most optimal mtry by randomly searching and repeated cross validation

```{r}
# Random Search

control <- trainControl(method="repeatedcv",number=10,repeats=3,search="random") # Setting the train control

set.seed(seed) # setting the seed

mtry <- sqrt(ncol(xTrain)) # Defining the mtry

rfRandom <- train(label~.,data=xTrain,trControl=control, method="rf",metric=metric,tuneLength=15)

print(rfRandom)

plot(rfRandom)



```

Now that we have done the training, let us try prediction on the validation set and check the model.

```{r}
xVal$rfRandom <- predict(rfRandom,newdata = xVal[,1:11])

confusionMatrix(xVal$label,xVal$rfRandom)

```

The model which we got after the random search was pretty neat. Let us try predicting on the test set

```{r}
batTestSet$rfRandom <- predict(rfRandom,newdata = batTestSet[,5:15])

table(batTestSet$rfRandom)

```
Thank Lord, this looks pretty neat. Let us now try the next method, which is Grid Search

### Step 4.b : Grid Search

In this method, we define a grid of parameters to try

```{r}

# Defining the control parameters

control <- trainControl(method = "repeatedcv",number=10, repeats=3,search = "grid")

# Setting the seed

set.seed(seed)

# Defining Tune grid

tunegrid <- expand.grid(.mtry=c(1:10))

# Running the model

rfGrid <- train(label~.,data=xTrain,method="rf",metric=metric,trControl=control,tuneGrid = tunegrid)

# Printing the results

print(rfGrid)

plot(rfGrid)

```
### JMJPFU
### 1-Feb-2017

Let us look at prediction on the validation set and the test set with the new model

```{r}
# Testing on the validation set
xVal$rfGrid <- predict(rfGrid,newdata = xVal[,1:11])

confusionMatrix(xVal$label,xVal$rfGrid)

```

The grid search results were a little wayward compared to the randome search results. Let us try testing on the test set also.

```{r}
batTestSet$rfGrid <- predict(rfGrid,newdata = batTestSet[,5:15])

table(batTestSet$rfGrid)

```

Cant say much about these predictions. Will have to visualize the same and then arrive at conclusions.

### Step 4.c : Tune using algorithm tools

Random forest algorithm provides its native tuning methods. Let us try that out now.

```{r}
set.seed(seed)
bestmtry <- tuneRF(xTrain[,1:11],xTrain$label,stepFactor = 1.5,improve = 1e-5,ntree=500 )
print(bestmtry)

```

So as seen, mtry = 4 got some good results. It was in agreement to the results we got earlier.

Now let us try out the manual parameter search methods.

#### Manual tuning

```{r}
# Manual search

control <- trainControl(method="repeatedcv",number=10,repeats=3,search="grid")

tunegrid <- expand.grid(.mtry=c(sqrt(ncol(xTrain))))

modellist <- list()

for(ntree in c(1000,1500,2000,2500)){
  
  set.seed(seed)
  
  fit <- train(label~.,data=xTrain,method="rf",metric=metric,trControl=control,tuneGrid = tunegrid,ntree=ntree)
  
  key <- toString(ntree) # Making a string for indicating the model list
  
  modellist[[key]] <- fit
  
  
  
  
}

# Comparing results

results <- resamples(modellist)
summary(results)
dotplot(results)
```

With the custom method let us try prediction on the validation set.

```{r}
xVal$rfCustom <- predict(modellist$`1000`,newdata = xVal[,1:11])

confusionMatrix(xVal$label,xVal$rfCustom)

```

Let us try out some prediction on the test set also.

```{r}

batTestSet$rfCustom <- predict(modellist$`1000`,newdata = batTestSet[,5:15])

table(batTestSet$rfCustom)

```

### 4.d : Extending Caret with custom made functions

```{r}
customRF <- list(type="Classification",library="randomForest",loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry","ntree"),class = rep("numeric",2),label=c("mtry","ntree"))
customRF$grid <- function(x,y,len=NULL,search="grid"){}
customRF$fit <- function(x,y,wts,param,lev,last,weights,classProbs,...){
  
  randomForest(x,y,mtry=param$mtry,ntree=param$ntree,...)
}

customRF$predict <- function(modelFit,newdata,preProc=NULL,submodels=NULL)
  predict(modelFit,newdata)

customRF$prob <- function(modelFit,newdata,preProc=NULL,submodels=NULL)
  predict(modelFit,newdata,type="prob")

customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes



```

Let us use these in our new call to the RF function

```{r}
#train model

control <- trainControl(method="repeatedcv",number = 10, repeats=3)

tunegrid <- expand.grid(.mtry=c(1:15),.ntree=c(1000,1500,2000,2500))

set.seed(seed)

newCustom <- train(label~.,data=xTrain,method=customRF,metric=metric,tuneGrid=tunegrid,trControl=control)
summary(custom)
plot(custom)
```



