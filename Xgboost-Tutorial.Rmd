---
title: "JMJFPU-Patient"
output: html_notebook
---

# JMJPFU
### 24-Mar-2017


### This is a great work extracted from the below website. Need to explore the details more and learn from it

http://jamesmarquezportfolio.com/


#################







Walk-through Of Patient No-show Supervised Machine Learning Classification With XGBoost In R¶

This walk-through is a project I've been working on for some time to help improve the missed opportunity rate (no-show rate) for medical centers. It demonstrates the entire machine learning process starting with extracting datasets from an SQL server and loading them directly into your R environment, performing exploratory analyses and creating visualizations to gain insight, engineering new features, model tuning and training, and finally measuring the model's performance. Finally, a web application is demonstrated at the end of this article which I developed to present the patient prediction results to the agency's leaders and frontline staff. I would like to share my results and methodology as a guide to help others starting their project or to assist others with improving upon my results.

If you enjoy this article, please leave a comment at the end if you have any questions about my process or if you have any suggestions. Thank you for reading!

The Problem¶
Patients that do not show up for their appointment without advanced warning is an issue that negatively affects the Bronx VAMC's 191,000+ veteran patient population and the facility because each no-show lowers access (increases wait times) and is a missed billing opportunity. This is a substantial problem because the facility's average 17.5% no-show rate, which consists of roughly 450,000 scheduled appointments each year, has never dropped below the national 10% goal. This project is an attempt to lower the number of patients that no-show by using the XGBoost implementation of the gradient boosting machine learning algorithm. This project is on-going as new features are continuously being tested to increase the model's accuracy.

ALL 2 FY 1 FY
FY15
FY16
10% National Goal
Missed Ops Rate
No-show Rate Trends
Missed Ops Rate
FY16 Avg.
FY15 Avg.
Jan '15
Jan '16
Oct '14
Apr '15
Jul '15
Oct '15
Apr '16
Jul '16
Oct '16
0%
5%
10%
15%
20%
25%
Results Summary¶
Setting the probability threshold to 0.34 results in a true positive rate of 0.8220 (82.2%) and a true negative rate of 0.9567 (95.6%). However, if management deems a higher false positive rate as acceptable, a threshold of 0.22 could be used which would provide a true positive rate of 0.9026 (90.2%) and true negative rate of 0.9220 (92.2%).

Features or Explanatory Variables Used¶
1. Clinic No-show Rate	Patient's no-show rate by clinic.
2. Clinic cancellation rate	Patient's cancellation rate by clinic.
3. New Patient to Clinic	Has patient visited clinic in past 24 months?
4. Department No-show Rate	Patient's no-show rate by department.
5. Department Cancellation Rate	Patient's cancellation rate by department.
6. New Patient to Department	Has patient visited department in past 24 months?
7. Appointment Lead Time	Number of days patient is waiting for appointment.
8. Days Since Last Appointment	Number of days since patient's most recent past appointment.
9. Consecutive No-shows By Patient	Number of most recent consecutive no-shows patient has accrued.
10. Consecutive No-shows By Clinic	Number of most recent consecutive no-shows patient has accrued by each clinic.
11. Appointment Sum Per Day	Number of appointments patient has on the same day of predicted appointment.
12. Appointment Sum	Patient's appointment sum in past 12 months.
13. Consult Sum	Patient's consult sum in past 12 months.
14. Admission Sum	Number of patient admissions in past 12 months.
15. Department	Appointment's department.
16. Length of Appointment	Appointment's duration in minutes.
17. Appointment's Month	Appointment's month (Jan, Feb, etc.).
18. Appointment's Weekday	Appointment's Weekday (Mon, Tues, etc.).
19. Appointment's Hour	Appointment's hour (8am, 9am, etc.).
20. Consult Lead Time	Number of days elapsed since consult requested.
21. Gender	Patient's gender.
22. Age	Patient's age.
23. Marital Status	Married or single.
24. Race	Patient's race.
25. Address	Patient's address (longitude & latitude).
26. Homeless	Patient diagnosed as homeless.
27. Substance Abuse	Patient diagnosed as a substance abuser.
28. Low Income	Patient diagnosed as having low income.
Loading Required Packages¶
The first step is to install and load the following R packages which are required to perform each step in this project. Package descriptions were sourced from their respective reference manual which can be found at https://cran.r-project.org/web/packages/.

library(RODBC)   # Used to establish a connection with Microsoft Server
library(plyr)    # Tools for splitting, applying and combining data
library(dplyr)   # Functions to simplify data manipulation
library(data.table) # Functions to simplify data manipulation
library(caret)   # Functions to streamline the model training process for complex classification problems
library(e1071)   # Misc. statistics functions required for caret package
library(Matrix)  # Creates sparse matrices
library(xgboost) # An efficient and scalable implementation of gradient boosting framework
library(Metrics) # Evaluation metrics for machine learning
library(ROCR)    # Used to visualize the performance of scoring classifiers
library(pROC)    # Used to display and analyze ROC curves
library(sp)      # Classes and methods for spatial data
library(ggmap)   # Create spatial visualizations with ggplot2
library(ggplot2) # Create elegant data visualizations using the grammar of graphics
library(doParallel) # Provides parallel processing
n.cores <- detectCores() # Identify the machine's number of cores
registerDoParallel(n.cores) # Used to train model across multiple cores in parallel
n.cores <- getDoParWorkers()
paste(n.cores, 'workers utilized')
'8 workers utilized'
Building The Datasets With SQL¶
All database table and column names have been given aliases for security reasons. In this next step, we will gather a period of two years of historical appointment information as well as patient demographic information from VHA's Corporate Data Warehouse. We will connect R directly to Microsoft SQL Server via an ODBC connection using the RODBC package. We will use Structured Query Language (SQL) to pull the information from 11 tables. We will set three variables; start.date, end.date, and station, which allows us to reuse the same dates and facility number across each query.

We will store each table in its own data frame and combine them later because R provides us more control joining datasets with the dplyr package instead of SQL. Also, it makes writing SQL statements much easier when writing many small queries instead of one large complex and slow SQL query.

# Set variables to use across all SQL queries
start.date <- '2015-01-01'
end.date   <- '2017-01-01'
station    <- '526'

channel <- odbcConnect("Name Of ODBC Connection") # Establish connection with Microsoft SQL Server
# Gather apppointment and patient demographic information
appointments <- sqlQuery(channel, paste("
SELECT c.LocationType
      ,c.DivisionID
      ,b.Deceased
      ,b.TestPatientFlag
      ,c.NoncountClinicFlag
      ,a.LengthOfAppointment
      ,g.AppointmentStatusID
      ,a.CancelNoShowCode
      ,a.AppointmentDateTime
      ,a.CancelDateTime
      ,a.PatientID
      ,b.Gender
      ,b.MaritalStatus
      ,b.Age
      ,b.Race
      ,b.GISLatitude AS Lat
      ,b.GISLongitude AS Lon
      ,DATEDIFF(DAY, a.AppointmentMadeDate, a.AppointmentDateTime) AS ApptLeadTime
      ,f.StopCode AS Department
      ,f.StopCodeName
      ,c.LocationName
      ,a.Consult
      ,DATEDIFF(DAY, d.FileEntryDateTime, a.AppointmentDateTime) AS ConsultLeadTime
      ,LEFT(b.PatientName, 1) + RIGHT(b.PatientSSN, 4) AS PatientInfo
      ,b.PatientName
      ,b.PhoneCellular
      ,b.PhoneResidence
FROM Appointment a
  INNER JOIN Patient b
    ON a.PatientID = b.PatientID
  INNER JOIN Location c
    ON a.LocationID = c.LocationID
  LEFT JOIN Consult d
    ON a.ConsultID = d.ConsultID
  LEFT JOIN ConsultReason e
    ON d.ConsultID = e.ConsultID
  INNER JOIN StopCode f
    ON c.PrimaryStopCodeID = f.StopCodeID
  LEFT JOIN Outpt.Visit g
    ON a.VisitID = g.VisitID
WHERE a.Station = ", station, "
  AND a.AppointmentDateTime >= '", start.date, "'
  AND a.AppointmentDateTime  < '", end.date, "'
ORDER BY a.PatientID, a.AppointmentDateTime", sep=''), stringsAsFactors=FALSE)

# Gather patient information that identifies homelessness using ICD 10 codes
homeless <- sqlQuery(channel, paste("
SELECT PatientID
      ,ICD10ID
FROM Outpt.Diagnosis
WHERE Station = ", station, "
  AND VisitDateTime >= '", start.date, "'
  AND VisitDateTime  < '", end.date, "'
  AND ICD10ID IN (1400858715, 1400858716, 1400858717, 1400858718, 1400858719,
                  1400858720, 1400858721, 1400858722, 1400858723, 1400858724)
ORDER BY PatientID", sep=''), stringsAsFactors=FALSE)

# Gather patient information that identifies substance abuse using ICD 10 codes
subabuse <- sqlQuery(channel, paste("
SELECT PatientID
      ,ICD10ID
FROM Outpt.Diagnosis
WHERE Station = ", station, "
  AND VisitDateTime >= '", start.date, "'
  AND VisitDateTime  < '", end.date, "'
  AND ICD10ID IN (1400434244, 1400434245, 1400434246, 1400434247, 1400434248,
                  1400434249, 1400434250, 1400434251, 1400434252, 1400434253,
                  1400434254, 1400434259, 1400858951, 1400858952)
ORDER BY PatientID", sep=''), stringsAsFactors=FALSE)

# Gather patient information that identifies low income using ICD 10 codes
lowincome <- sqlQuery(channel, paste("
SELECT PatientID
      ,ICD10ID
FROM Outpt.Diagnosis
WHERE Station = ", station, "
  AND VisitDateTime >= '", start.date, "'
  AND VisitDateTime  < '", end.date, "'
  AND ICD10ID = 1400858721
ORDER BY PatientID", sep=''), stringsAsFactors=FALSE)

# Gather inpatient patient information
admits <- sqlQuery(channel, paste("
SELECT PatientID
FROM Inpt.Inpatient
WHERE Station = ", station, "
  AND AdmitDateTime >= '", start.date, "'
  AND AdmitDateTime  < '", end.date, "'
ORDER BY PatientID", sep=''), stringsAsFactors=FALSE)
close(channel) # Close connection with database

# View the number of rows and columns for each data frame
dim(appointments)
dim(homeless)
dim(subabuse)
dim(lowincome)
dim(admits)
930239 27
43469 2
1021 2
403 2
8876 1
Preprocessing And Cleaning Data¶
Now that the raw datasets are stored in separate data frames, the next step is to clean and combine them. This step also requires a lot of domain specific knowledge. For example, we will remove all deceased patients because they will no longer be scheduling appointments and will negatively affect the model if left in. Also, we will remove patients flagged as test patients in the database. We will also remove clinics whose no-show performance is not monitored. Next, we'll remove inpatient appointments and appointments with data entry errors. Lastly, even though appointments canceled by the facility after the appointment date/time count toward the no-show rate, we'll remove those because they are out of the patient's control.

d <- appointments # Create working copy of appointments data frame

d <- subset(d, (DivisionID == 648)) # Get only James J. Peters patients
d <- subset(d, (Deceased == 'N')) # Remove deceased patients
d <- subset(d, is.na(TestPatientFlag)) # Remove test patients
d <- subset(d, (NoncountClinicFlag == 'N')) # Remove non-count clinics
d <- subset(d, (AppointmentStatusID == 21105) | (AppointmentStatusID == -1)) # Get only checked out and noshows/cancellations, exclude inpatient and incomplete encounters
d <- subset(d, !grepl('^CA?', d$CancelNoShowCode) | is.na(d$CancelNoShowCode)) # Remove appointments canceled by the clinic that are out of the patient's control

# Display number of rows that were dropped
dim(appointments) - dim(d)
173361 0
We dropped 173,361 rows that would have otherwise negatively affected our model.

Next, we'll combine the homeless, subabuse, and lowincome data frames to our working data frame d. We'll remove duplicate patient ID records by calling subset(x, !duplicated(PatientID)) on each data frame because each patient may have multiple entries, but we only need one instance for each unique patient. Then we assign them to their matching patient IDs in our working data frame.

# Remove duplicate patients from each data frame
h <- droplevels(subset(homeless, !duplicated(PatientID)))
s <- droplevels(subset(subabuse, !duplicated(PatientID)))
l <- droplevels(subset(lowincome, !duplicated(PatientID)))

# Join Homeless, SubstanceAbuse, and LowIncome data frames to working data frame
d$Homeless       <- h$ICD10ID[match(d$PatientID, h$PatientID)] # Assign ICD 10 code to Homeless feature by patient ID
d$SubstanceAbuse <- s$ICD10ID[match(d$PatientID, s$PatientID)]
d$LowIncome      <- l$ICD10ID[match(d$PatientID, l$PatientID)]
Visualizing the Label Column¶
Let's create our Label feature. This is the feature that we are trying to predict. It may also be called the outcome, response, or dependent variable depending in which field you operate. For this project, since we are trying to predict patients that will no-show, we'll assign a 1 to every instance that the patient either no-showed or canceled after their appointment date/time. We'll assign 0 to every case where the patient did show up. We want to train our model to understand which features contribute to every case where our Label feature is equal to 1.

# Label feature 1 = patient no-showed or canceled after appointment date/time
# Look for 'N' which is a no-show OR if the cancel datetime is after the appt datetime AND it is a patient cancellation
d$Label <- ifelse(grepl('N', d$CancelNoShowCode) | ((d$CancelDateTime > d$AppointmentDateTime) & grepl('PC', d$CancelNoShowCode)), 1, 0) 
d$Label[is.na(d$Label)] <- 0 # Assign 0 to patient cancelations where time/date stamp was not saved in the database

# Proportions of each Label value
prop <- data.frame(round(prop.table(table(d$Label)), 4))
prop$Var1 <- as.integer(prop$Var1)
prop$y <- as.integer(table(d$Label))
prop[1, 1] <- 0
prop[2, 1] <- 1


# Plot both Label values
ggplot(d, aes(Label)) +
    geom_bar(fill=c('#00BFC4','#FF9999')) +
    scale_x_continuous(breaks=seq(0, 1)) +
    geom_text(data=prop, aes(x=Var1, y=y, label=Freq), vjust=-0.35, size=6.5, color="#000000")
 
This plot shows us that most patients, 86.03%, show up for their appointment while 13.97% no-show. For this project, we are most interested in predicting which patients are most likely to no-show rather than which patients will likely show.

Feature Engineering¶
Feature engineering is the process of creating new features that help the learning algorithm understand the problem. Features in machine learning are analogous to independent variables in statistical experiments.

VHA groups similar services into departments to determine if a patient is new to the department or if the patient is returning for a follow-up visit. We will use VHA's national guidelines to group the services together, which will also reduce the number of factors or categorical variables in this feature from 120 down to 51. Our goal is to find out if patients are not showing up for some departments more than others. Each service is assigned a 3-digit identifier that we'll use when assigning them to groups.

# Group departments
# The following statments will overwrite the existing 3-digit identifier with the group's name
d$Department[d$Department %in% c('203', '204')] <- "Audiology"
d$Department[d$Department %in% c('159', '436')] <- "Alternative"
d$Department[d$Department %in% c('211', '418')] <- "Amputation"
d$Department[d$Department %in% c('419', '420', '427', '434', '441')] <- "Anesthesia"
d$Department[d$Department %in% c('209', '217', '218', '220', '221', '229', '437', '438', '439')] <- "Blindrehab"
d$Department[d$Department %in% c('402', '413')] <- "Cardiothoracic"
d$Department[d$Department %in% c('231', '303', '311', '333', '334', '369')] <- "Cardiology"
d$Department[d$Department %in% c('163', '164', '165', '166', '167', '168', '169')] <- "Chaplain"
d$Department[d$Department %in% c('118', '119', '121', '122', '170', '171', '172', '173', '174',
                                 '175', '176', '177', '178', '182', '191', '347', '354', '680',
                                 '683', '685', '686')] <- "Homebased"
d$Department[d$Department %in% c('208', '222')] <- "Cwt"
d$Department[d$Department %in% c('180', '181')] <- "Dental"
d$Department[d$Department %in% c('101', '130')] <- "Emergency"
d$Department[d$Department %in% c('305', '306')] <- "Endo"
d$Department[d$Department %in% c('407', '408', '428', '718')] <- "Eye"
d$Department[d$Department %in% c('307', '321', '337')] <- "Gastroenterology"
d$Department[d$Department %in% c('102', '103', '131', '301', '309', '317', '324', '327', '329',
                                 '331', '336', '339', '340', '349', '394')] <- "GeneralMedicine"
d$Department[d$Department %in% c('401', '412', '416', '424', '429', '431', '432', '435')] <- "GeneralSurgery"
d$Department[d$Department %in% c('190', '318', '319', '320', '326', '351', '352', '353')] <- "Geriatric"
d$Department[d$Department %in% c('404', '426')] <- "Gyn"
d$Department[d$Department %in% c('308', '316', '330')] <- "Hema"
d$Department[d$Department %in% c('555', '556')] <- "HomelessEmployment"
d$Department[d$Department %in% c('504', '507', '508', '511', '522', '529', '590', '591', '592')] <- "Homeless"
d$Department[d$Department %in% c('108', '111')] <- "Laboratory"
d$Department[d$Department %in% c('520', '521', '156', '157', '502', '503', '505', '506', '509',
                                 '510', '512', '513', '514', '516', '519', '523', '524', '525',
                                 '533', '534', '535', '538', '539', '540', '547', '548', '550',
                                 '552', '553', '554', '557', '558', '560', '561', '562', '563',
                                 '564', '565', '566', '567', '568', '571', '572', '573', '574',
                                 '575', '576', '577', '578', '580', '581', '582', '583', '588',
                                 '589', '593', '594', '595', '596', '598', '599')] <- "MentalHealth"
d$Department[d$Department %in% c('313', '602', '603', '604', '606', '607', '608', '610', '611')] <- "Nephrology"
d$Department[d$Department %in% c('106', '126', '127', '128', '148', '315', '325', '335', '345', '346')] <- "Neurology"
d$Department[d$Department %in% c('109', '144', '145', '146', '148', '149', '158')] <- "Nuclear"
d$Department[d$Department %in% c('142', '147', '328', '332', '433')] <- "Nursing"
d$Department[d$Department %in% c('123', '124', '147', '708')] <- "Nutrition"
d$Department[d$Department %in% c('405', '409', '422')] <- "Orthopedics"
d$Department[d$Department %in% c('147', '160', '161')] <- "Pharmacy"
d$Department[d$Department %in% c('195', '196', '197', '198')] <- "Polytrauma"
d$Department[d$Department %in% c('322', '323', '338', '341', '342', '348', '350', '702', '704',
                                 '705', '709', '711')] <- "Primarycare"
d$Department[d$Department %in% c('417', '423', '425')] <- "Prosthetics"
d$Department[d$Department %in% c('104', '116', '148', '312')] <- "Pulmonary"
d$Department[d$Department %in% c('105', '110', '115', '148', '150', '151', '152', '153', '154',
                                 '155', '703')] <- "Radiology"
d$Department[d$Department %in% c('201', '216')] <- "Rehabphysician"
d$Department[d$Department %in% c('586', '587', '725', '726', '727', '728', '729', '730', '731')] <- "ResidentialRehab"
d$Department[d$Department %in% c('125', '147')] <- "SocialWork"
d$Department[d$Department %in% c('210', '215', '224', '225')] <- "SpinalCordInjury"
d$Department[d$Department %in% c('202', '205', '206', '207', '213', '214', '230', '240', '250')] <- "Therapy"
d$Department[d$Department %in% c('414', '430')] <- "Urology"
d$Department[d$Department %in% c('372', '373')] <- "Weightmgmt"
Okay, now that we have our departments grouped, let's see if some departments have more no-shows than others. It's important to view department no-shows by rate rather than by total no-shows because some departments are much larger than others. Looking at it simply by total no-shows, the Mental Health department would seem like they have the most no-shows. However, they have more no-shows because they have a much higher volume of appointments than any other department. That would not indicate whether there is some factor causing patients to no-show in Mental Health more than other departments. Let's find each department's no-show rate and plot them from highest to lowest.

dep <- data.frame(table(d$Department, d$Label)) # Create a data frame that aggregates and split the total shows and no-shows for each department
dep.no.shows <- subset(dep, Var2 == 1) # Create data frame with service's no-show aggregates
dep.shows    <- subset(dep, Var2 == 0) # Create data frame with service's show aggregates 

# Because there may be some departments that do not have no-shows, we cannot simply join the Freq column from dep.shows to dep.no.shows because the length of the columns may differ
dep.no.shows$Shows <- dep.shows$Freq[match(dep.no.shows$Var1, dep.shows$Var1)] # We use the match function to assign the show values from only those that are in the dep.no.shows data frame
dep.no.shows$Rate  <- dep.no.shows$Freq / (dep.no.shows$Freq + dep.no.shows$Shows)
dep.no.shows$Lab   <- paste(round(dep.no.shows$Rate * 100, 1), '%', sep='')

ggplot(data=dep.no.shows, aes(x=(reorder(Var1, Rate)), y=Rate)) +
    ggtitle("Department No-show Rates") +
    xlab("Departments") +
    geom_bar(stat="identity", fill="#FF9999") +
    geom_text(data=dep.no.shows, aes(x=Var1, y=Rate, label=Lab), vjust=0.35, hjust=1.05, size=2.5, color="#000000") +
    coord_flip() # flip the position of the x and y axes

rm(dep, dep.no.shows, dep.shows) # Remove temp data frames
 
We can see that the emergency department has an extremely low no-show rate, which makes sense. However, it's the 9th largest producer of appointments out of 51 departments. That's significant because it means patients scheduled in the Emergency department will most likely always show up. We'll see later that the model realizes this fact as well. We can also use this information to find best practices from departments that have low no-show rates with high volume such as Prosthetics.

Our training dataset will consist of 12 months of data, but the next three features will require us to use data during the period 12 - 24 months. We can drop the 12 - 24-month period after we create them.

The first feature will determine if the patient has not visited the same department within a 24-month period, and thus be considered a new patient. We'll do this by first creating a new column that concatenates each patient's unique ID and their scheduled appointment's department only if they had a checked-out visit. Then, we'll loop over the column and assign a Yes if no duplicates are found and No if it does find one or more duplicates.

# New to department
d$NewPatientID <- ifelse(is.na(d$CancelNoShowCode), paste(d$PatientID, d$Department, sep=''), NA)
d$NewPatientDepartment <- ifelse(!duplicated(d$NewPatientID), 'Yes', 'No') # Yes = new patient
We'll use the same concept to determine if the patient has not visited the same clinic in a 24-month period. Clinics are subcomponents of each department.

# New to clinic
d$NewClinicID <- ifelse(is.na(d$CancelNoShowCode), paste(d$PatientID, d$LocationName, sep=''), NA)
d$NewPatientClinic <- ifelse(!duplicated(d$NewClinicID), 'Yes', 'No') # Yes = new patient
The next feature we'll create is the number of days that are between each patient's past most recent appointment and the current appointment. We want to find out if a long time has elapsed since the patient has last visited the medical center.

# Days since last appointment
d$ApptDate <- as.numeric(as.Date(d$AppointmentDateTime))
d$DaysSinceLastAppt <- ave(d$ApptDate, d$PatientID, FUN=function(x) c(0, diff(x)))
Now that we're finished creating the previous three features, we can subset our data to get only the most recent 12 months to use for training.

date <- as.POSIXct("2016-01-01 00:00:01", tz="America/New_York") # Set date and time to subset
d <- subset(d, AppointmentDateTime > date)
Now that we have our training dataset, let's group similar race ID's into five race groups and one group for unknown race.

# Group Race IDs
d$Race[d$Race %in% c(1698)]             <- 0 # Native
d$Race[d$Race %in% c(1694, 1695, 1691)] <- 1 # Asian
d$Race[d$Race %in% c(1693, 1699, 1701)] <- 2 # Black
d$Race[d$Race %in% c(1696)]             <- 3 # Hispanic
d$Race[d$Race %in% c(1689, 1697)]       <- 4 # White
d$Race[d$Race %in% c(-1, 0, 1692, 1700, 1690, 1702)] <- 5 # Unknown
Next, let's reduce the amount of married and non-married factors down to just married or single. The data has several types of non-married such as divorced, single, and widow. We are most interested in only married and single because we want to find out which patients have a loved one at home that can help them.

# Patient's marital status
d$MaritalStatus <- ifelse(grepl('MARRIED', d$MaritalStatus), 'M', 'S') # Code values with married as 'M' and all the others as 'S'
For the next feature, we'll assign each patient into one of four age buckets; below 21, 21 - 38, 39 - 60, and above 60.

# Group ages into four categories
d$Age <- as.character(d$Age)
d$Age <- ifelse(d$Age < 21, 'Below21',
                 ifelse(d$Age >= 21 & d$Age <= 38, '21-38',
                        ifelse(d$Age >=39 & d$Age <= 60, '39-60', 'Above60')))
The next feature, the sum of each patient's appointments, will be a feature on its own and be used to calculate other features, such as each patient's cancelation rates and no-show rates. First, we must sum each patient's appointments grouped by their unique ID. We'll use the aggregate function to create a separate data frame with the sums for each unique patient ID, and then assign those sums back to our working data frame using the same unique patient ID. We'll use this same technique for several other features as well.

# Sum patient's appointments
d$Appt    <- 1 # Assign 1 to new column to be used for aggregation
appt.sums <- aggregate(d$Appt, by=list(PatientID=d$PatientID), FUN=sum) # Sum appointments by patient's ID
d$ApptSum  <- appt.sums$x[match(d$PatientID, appt.sums$PatientID)] # Merge patient appointment sums to working data frame by Patient's ID
d$ApptSum[is.na(d$ApptSum)] <- 0
rm(appt.sums) # Remove temp data frame
Next, we'll sum the total number of consults each patient had in our 12-month training set period. A consult is a digital request when a patient sees a specialist for the first time. Follow-up visits with the same specialist do not require consults. The dataset contains three values, NA, -1 and a value greater than 0. All values greater than 0 are the consult request ID. We'll transform all values less than 0 or NA to 0, and every other value to 1.

# If appointment is for a consultation or return follow-up
d$Consult <- ifelse(is.na(d$Consult) | d$Consult < 0, 0, 1) # 1 = Consult

# Aggregate patient's consults
consult.sums <- aggregate(d$Consult, by=list(PatientID=d$PatientID), FUN=sum) # Sum consults by patient
d$ConsultSum <- consult.sums$x[match(d$PatientID, consult.sums$PatientID)] # Merge consult sums to working data frame by patient's ID
rm(consult.sums)
Next, we'll calculate each patient's no-show rates by department. First, we'll concatenate each patient's ID and the department to make a new DepartmentID. We'll aggregate the no-shows and total appointments based on the new DepartmentID we created. Then, we'll use that ID to assign our aggregates back to our working data frame. Once we have the total no-shows and total appointments by our new ID, we can then divide them to get the no-show rate.

# Department No-show Rate
d$DepartmentSID <- paste(d$PatientID, d$Department, sep='')
d$DepartmentAppt     <- 1 # Assign 1 to new column to be used for aggregation
department.appt.sums <- aggregate(d$DepartmentAppt, by=list(DepartmentSID=d$DepartmentSID), FUN=sum) # Sum appointments by patient's ID
d$DepartmentApptSum  <- department.appt.sums$x[match(d$DepartmentSID, department.appt.sums$DepartmentSID)] # Merge patient appointment sums to working data frame by Patient's ID
rm(department.appt.sums) # Remove temp data frame

noshow.department.sums <- aggregate(d$Label, by=list(DepartmentSID=d$DepartmentSID), FUN=sum) # Sum missed ops by patient
d$DepartmentNoshowSum  <- noshow.department.sums$x[match(d$DepartmentSID, noshow.department.sums$DepartmentSID)]
d$DepartmentNoshowRate <- d$DepartmentNoshowSum / d$DepartmentApptSum
rm(noshow.department.sums) # Remove temp data frame
We'll use the same technique to calculate each patient's no-show rate by clinic.

# Clinic No-show Rate
d$ClinicSID <- paste(d$PatientID, d$LocationName, sep='')
d$ClinicAppt     <- 1 # Assign 1 to new column to be used for aggregation
clinic.appt.sums <- aggregate(d$ClinicAppt, by=list(ClinicSID=d$ClinicSID), FUN=sum) # Sum appointments by patient's ID
d$ClinicApptSum  <- clinic.appt.sums$x[match(d$ClinicSID, clinic.appt.sums$ClinicSID)] # Merge patient appointment sums to working data frame by Patient's ID
rm(clinic.appt.sums) # Remove temp data frame

noshow.clinic.sums <- aggregate(d$Label, by=list(ClinicSID=d$ClinicSID), FUN=sum) # Sum missed ops by patient
d$ClinicNoshowSum  <- noshow.clinic.sums$x[match(d$ClinicSID, noshow.clinic.sums$ClinicSID)]
d$ClinicNoshowRate <- d$ClinicNoshowSum / d$ClinicApptSum
rm(noshow.clinic.sums) # Remove temp data frame
Next, we'll use the same DepartmentID and ClinicID to find each patient's cancelation rate by department and clinic. This is the total amount of times the patient acted responsibly by canceling their appointment before their scheduled date/time relative to the total amount of appointments they've had.

# Assign 1 to new column where appointments were canceled prior to the scheduled date/time
d$Canx <- ifelse(grepl('PC', d$CancelNoShowCode) & d$CancelDateTime < d$AppointmentDateTime, 1, 0)
d$Canx[is.na(d$Canx)] <- 0

# Department Cancelation Rate
cancel.department.sums <- aggregate(d$Canx, by=list(DepartmentSID=d$DepartmentSID), FUN=sum) # Sum missed ops by patient
d$DepartmentCancelSum  <- cancel.department.sums$x[match(d$DepartmentSID, cancel.department.sums$DepartmentSID)]
d$DepartmentCancelRate <- d$DepartmentCancelSum / d$DepartmentApptSum
rm(cancel.department.sums) # Remove temp data frame
 # Clinic Cancelation Rate
cancel.clinic.sums <- aggregate(d$Canx, by=list(ClinicSID=d$ClinicSID), FUN=sum) # Sum missed ops by patient
d$ClinicCancelSum  <- cancel.clinic.sums$x[match(d$ClinicSID, cancel.clinic.sums$ClinicSID)]
d$ClinicCancelRate <- d$ClinicCancelSum / d$ClinicApptSum
rm(cancel.clinic.sums) # Remove temp data frame
Next, we'll sum the number of times each patient has been admitted into the hospital in our 12-month training set period. We want to get an idea of how sick each patient is. The more times a patient is admitted, the sicker they are likely to be.

# Total Patient Admissions
admits$Count   <- 1 # Assign 1 to new column to be used for aggregation
admission.sums <- aggregate(admits$Count, by=list(PatientID=admits$PatientID), FUN=sum) # Sum admissions by patient's ID
d$AdmissionSum <- admission.sums$x[match(d$PatientID, admission.sums$PatientID)] # Merge admission sums to working data frame by patient's ID
d$AdmissionSum[is.na(d$AdmissionSum)] <- 0
rm(admission.sums) # Remove temp data frame
Next, we'll calculate the number of consecutive no-shows each patient has accrued up to their appointment date. We'll calculate them based on each patient's ID and by the ClinicID.

# Most Recent Consecutive No-shows by Patient ID
d <- setDT(d)[, ConsecNoshows := seq(.N) * Label, by=.(PatientID, rleid(Label))]
# Shifts all results down one row
d <- d[, ConsecNoshows := shift(ConsecNoshows, fill=0), by=PatientID]
 # Most Recent Consecutive No-shows by Clinic ID
d <- setDT(d)[, ConsecNoshowsClinic := seq(.N) * Label, by=.(ClinicSID, rleid(Label))]
# Shifts all results down one row
d <- d[, ConsecNoshowsClinic := shift(ConsecNoshowsClinic, fill=0), by=ClinicSID]
Next, we'll create three separate features by extracting date parts from the appointment's date and time. We'll extract the appointment's month, the weekday, and the hour.

# Extract date/time parts from appointment date/time
d$ApptMonth <- format(d$AppointmentDateTime, format='%B') # Appointment Month
d$ApptWeekday <- format(d$AppointmentDateTime, format='%A') # Appointment Weekday (Mon, Tues, Weds)
d$ApptHour  <- format(d$AppointmentDateTime, format='%H') # Appointment hour 1 - 24
Our next feature will be the total number of appointments each patient has on the same date as the predicted appointment. We'll use the same technique by creating a new ID based on the patient's ID and the appointment date. Then we'll aggregate based on the new PatientApptDateID.

# Count sum of appointments in each day
d$PatientApptDateID <- paste(d$PatientID, format(d$AppointmentDateTime, format='%m%d%Y'), sep='') # Create Unique ID with patient's ID and appointment date

d$ApptSumPerDay    <- 1 # Assign 1 to new column to be used for aggregation
apptsumperday.sums <- aggregate(d$ApptSumPerDay, by=list(PatientApptDateID=d$PatientApptDateID), FUN=sum) # Aggregate by unique PatientApptDateID
d$ApptSumPerDay    <- apptsumperday.sums$x[match(d$PatientApptDateID, apptsumperday.sums$PatientApptDateID)] # Assign aggregates back to d$ApptSumPerDay by PatientApptDateID
d$ApptSumPerDay[is.na(d$ApptSumPerDay)] <- 0
rm(apptsumperday.sums)
Next, we'll fill in missing values in the LengthOfAppointment feature with 'None'. We'll fill in missing values in the Homeless, SubstanceAbuse, and LowIncome columns with 0's and transform all others into 1's.

# Fill length of appointments that are missing values with 'none'
d$LengthOfAppointment <- ifelse(is.na(d$LengthOfAppointment), 'None', d$LengthOfAppointment)

# Patients identified as homeless
d$Homeless <- ifelse(is.na(d$Homeless), 0, 1)

# Patients identified as substance abusers
d$SubstanceAbuse <- ifelse(is.na(d$SubstanceAbuse), 0, 1)

# Patients identified as receiving low income
d$LowIncome <- ifelse(is.na(d$LowIncome), 0, 1)
Next, let's look to see if patients are more likely to no-show based on their home address' geolocation. If so, we can create a new categorical feature that groups patients into different geo areas. We'll use the ggmap package to plot patient's home addresses' longitude and latitude coordinates on top of a map of New York. We'll color the shows in blue and the no-shows in red to differentiate between the two groups.

First, we're going to separate the no-shows from shows and save them in two different data frames. Then, we'll call the ggmap() function and pass in our longitude and latitude vectors from the no-shows' data frame.

labels     <- subset(d, Label == 1) # Get only missed opportunities
non.labels <- subset(d, Label == 0) # Get completed appointments

# Assign longitude and latitude of center of map we're going to download with the get_map() function
center <- c(-73.895283, 40.845000)

# Coordinates of Bronx VAMC to plot its location on the map
lat <-  40.868521
lon <- -73.905272

# Plot to find high concentrations of no-shows
ggmap(get_map(center, zoom=13, source='stamen', maptype='toner-lines'), extent="device") +
    ggtitle("Shows and No-shows by Patient's Addresses") +
    # Plot no-shows
    stat_density2d(data=labels,
                   aes(x=Lon, y=Lat, fill="No-shows", alpha=..level..),
                   size=2, bins=4, geom="polygon") +
    # Plot shows
    stat_density2d(data=non.labels,
                   aes(x=Lon, y=Lat, fill="Shows", alpha=..level..),
                   size=2, bins=4, geom="polygon") +
    # Add point and label for Bronx VAMC
    geom_point(aes(x=-73.905272, y=40.867021), col="black", alpha=1, size=3, pch=15) +
    geom_text(mapping=aes(x=-73.905272, y=40.865500, label="VAMC"), size=4)

rm(labels, non.labels) # Remove temp data frame from environment
 
As we can see, there are not many areas of high concentration no-shows that do not also have high concentrations of shows. After much trial and error, I decided to simply transform the patient's longitude and latitude by summing each other into one feature.

# Combine patient's home longitude and latitude to create one feature
d$Address <- d$Lon + d$Lat

d$Address[is.na(d$Address)] <- round(mean(d$Address, na.rm=TRUE), 0) # Assign average address to all fields missing Address
The next features were ultimately not included after much trial and error. They were features describing weather conditions for each date, such as average temperature, rain, and snow. The dataset was gathered from http://Weatherunderground.com. After many tests, it was determined that they introduced too much noise to the model and decreased accuracy. I decided to drop them from the training set.

# Adding weather conditions (avg temp, rain, snow) did not improve the model's performance
# w <- read.csv(file="historical_weather.csv", header=TRUE) # Import weather gathered from Weatherunderground.com
# w$Date <- as.Date(w$Date, "%Y-%m-%d")
# d <- inner_join(d, w, by="Date")
Data Cleanup¶
The final step cleans up the data frame by eliminating every column that will not be used as a feature when training the model. Then, we convert categorical variables to factors and our target variable Label to integer, which is a requirement of the XGBoost algorithm.

# Remove unused features and reorder features in data frame
d <- d[, c('Label', 'Gender', 'Age', 'MaritalStatus', 'Race', 'Homeless',
           'SubstanceAbuse', 'LowIncome', 'Address', 'ApptLeadTime', 'ApptMonth',
           'ApptWeekday', 'ApptHour', 'ApptSumPerDay', 'LengthOfAppointment',
           'ConsultLeadTime', 'Department', 'NewPatientDepartment', 'NewPatientClinic',
           'DaysSinceLastAppt', 'DepartmentNoshowRate', 'ClinicNoshowRate',
           'DepartmentCancelRate', 'ClinicCancelRate', 'ConsecNoshows', 'ConsecNoshowsClinic',
           'ApptSum', 'ConsultSum', 'AdmissionSum')]

# Convert features to correct data types
d$Label          <- as.integer(d$Label)
d$Gender         <- factor(d$Gender)
d$MaritalStatus  <- factor(d$MaritalStatus)
d$Age            <- factor(d$Age)
d$Department     <- factor(d$Department)
d$ApptMonth       <- factor(d$ApptMonth)
d$ApptWeekday    <- factor(d$ApptWeekday)
d$ApptHour       <- factor(d$ApptHour)
d$NewPatientDepartment <- factor(d$NewPatientDepartment)
d$NewPatientClinic <- factor(d$NewPatientClinic)
d$Homeless       <- factor(d$Homeless)
d$SubstanceAbuse <- factor(d$SubstanceAbuse)
d$LowIncome      <- factor(d$LowIncome)
d$Race           <- factor(d$Race)
d$LengthOfAppointment <- factor(d$LengthOfAppointment)

d <- data.frame(d) # Convert data.table back to data frame
str(d) # View feature's data types
 'data.frame':  386404 obs. of  29 variables:
 $ Label               : int  0 1 0 0 0 0 1 1 1 0 ...
 $ Gender              : Factor w/ 2 levels "F","M": 2 2 2 2 2 2 2 2 2 2 ...
 $ Age                 : Factor w/ 4 levels "21-38","39-60",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ MaritalStatus       : Factor w/ 2 levels "M","S": 2 2 2 2 2 2 2 2 2 2 ...
 $ Race                : Factor w/ 5 levels "1","2","3","4",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ Homeless            : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...
 $ SubstanceAbuse      : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...
 $ LowIncome           : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...
 $ Address             : num  -33.1 -33.1 -33.1 -33.1 -33.1 ...
 $ ApptLeadTime        : int  61 3 14 56 0 105 16 12 5 4 ...
 $ ApptMonth           : Factor w/ 12 levels "April","August",..: 5 5 5 4 4 4 4 4 8 8 ...
 $ ApptWeekday         : Factor w/ 7 levels "Friday","Monday",..: 2 5 2 6 6 7 5 2 7 2 ...
 $ ApptHour            : Factor w/ 24 levels "00","01","02",..: 12 15 13 12 13 10 12 11 12 14 ...
 $ ApptSumPerDay       : num  1 1 1 2 2 1 1 1 1 1 ...
 $ LengthOfAppointment : Factor w/ 27 levels "10","100","120",..: 10 23 27 17 17 27 23 10 23 27 ...
 $ ConsultLeadTime     : int  NA NA NA NA NA NA NA NA NA NA ...
 $ Department          : Factor w/ 51 levels "107","199","212",..: 6 34 34 34 34 6 34 6 34 34 ...
 $ NewPatientDepartment: Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 1 1 1 1 ...
 $ NewPatientClinic    : Factor w/ 2 levels "No","Yes": 2 1 1 1 1 1 1 1 1 1 ...
 $ DaysSinceLastAppt   : num  4 3 18 15 0 8 8 4 2 19 ...
 $ DepartmentNoshowRate: num  0.273 0.167 0.167 0.167 0.167 ...
 $ ClinicNoshowRate    : num  0 0.5 0.5 0 0.5 ...
 $ DepartmentCancelRate: num  0.182 0.146 0.146 0.146 0.146 ...
 $ ClinicCancelRate    : num  0 0.333 0.333 0 0.333 ...
 $ ConsecNoshows       : num  0 0 1 0 0 0 0 1 2 3 ...
 $ ConsecNoshowsClinic : num  0 0 1 0 0 0 0 0 1 2 ...
 $ ApptSum             : num  111 111 111 111 111 111 111 111 111 111 ...
 $ ConsultSum          : num  5 5 5 5 5 5 5 5 5 5 ...
 $ AdmissionSum        : num  0 0 0 0 0 0 0 0 0 0 ...
Handling Missing Data¶
Now that we've eliminated the columns that we aren't interested in, it's important to either correct or remove all missing values from the dataset completely because most machine learning algorithms do not work well with missing values. Now, let's see how many missing values are in each column of our data frame.

sapply(d, function(x) sum(is.na(x)))
Label
0
Gender
3
Age
8
MaritalStatus
0
Race
0
Homeless
0
SubstanceAbuse
0
LowIncome
0
Address
0
ApptLeadTime
0
ApptMonth
0
ApptWeekday
0
ApptHour
0
ApptSumPerDay
0
LengthOfAppointment
0
ConsultLeadTime
354038
Department
0
NewPatientDepartment
0
NewPatientClinic
0
DaysSinceLastAppt
0
DepartmentNoshowRate
0
ClinicNoshowRate
0
DepartmentCancelRate
0
ClinicCancelRate
0
ConsecNoshows
0
ConsecNoshowsClinic
0
ApptSum
0
ConsultSum
0
AdmissionSum
0
We see that there are missing values in three columns, Gender, Age, and ConsultLeadTime. Since this is patient data that may also be missing from patients that we are going to be predicting in the future, we'll use a method to fill the missing values instead of removing them because we can't simply remove them at prediction time. For Age and Gender, we're going to assign the most frequent category to all rows with missing values, which are '>60' and 'M' respectively. For ConsultLeadTime, consults that are not linked to an appointment have missing values. We're going to assign 0 to these because consults that are linked to an appointment have been assigned a value of 1.

d$Gender[is.na(d$Gender)] <- names(sort(table(d$Gender), decreasing=TRUE)[1]) # Assign most frequent gender

d$Age[is.na(d$Age)] <- names(sort(table(d$Age), decreasing=TRUE)[1]) # Assign most frequent age

d$ConsultLeadTime[is.na(d$ConsultLeadTime)] <- 0 # Fill in 0 for appointments not linked to a consult
Center And Scale Data¶
Some models perform better, such as the K-nearest neighbor algorithm, when every numeric variable is centered and scaled. This prevents extremely high or low values from skewing the model's predictive performance. However, in our case, the XGBoost algorithm is not affected much by outliers and performed better when not centering and scaling our training dataset.

# These functions are from the caret package.
# preProcValues <- preProcess(d[, c(-1)], method=c("center", "scale"))
# dfTransformed <- predict(preProcValues, d)
# print(preProcValues)
# summary(dfTransformed)
Splitting The Data Into Train And Test Sets¶
Now that we have our complete dataset we need to split it into two separate sets; one for training the model and one for testing the model. We can use the caret package's createDataPartition function to create a training dataset consisting of 75% of our original dataset and a testing dataset consisting of 25% of our original dataset. The function splits the data by performing a stratified random sample ensuring that the proportions of our Label feature remain the same in both the training and testing sets.

Next, we'll create new vectors with only the Label feature from both the train and test sets. We'll use these vectors later to train our model and to test its performance.

# Create a stratified random sample to create train and test sets.
set.seed(300) # Set the seed to create reproducible train and test sets.
trainIndex   <- createDataPartition(d$Label, p=0.75, list=FALSE, times=1) # 75% train / 25% test split
train        <- d[ trainIndex, ]
test         <- d[-trainIndex, ]

# Create label vectors
train.label  <- train$Label
test.label   <- test$Label
One-Hot Encoding¶
An important aspect of most machine learning algorithms and XGBoost is the requirement that every feature be numeric. We can use a process called one-hot encoding to transform our categorical features into numeric features. Each category will become its own new binary feature with either a 1 to designate its presence, or a 0 to designate its non-presence. We can use caret's sparse.model.matrix function to automatically transform our entire data frame in one step. We'll transform both our train and test datasets.

# Create sparse matrixes and perform One-Hot Encoding to create dummy variables
dtrain  <- sparse.model.matrix(Label ~ .-1, data=train)
dtest   <- sparse.model.matrix(Label ~ .-1, data=test)

dim(dtrain)
dim(dtest)
289803 145
96601 145
Tune Hyperparameters Using The caret Package¶
The XGBoost algorithm requires several hyperparameter values before running efficiently. Selecting different values produces either better or worse performance when evaluating the model. Due to time constraints, it can be difficult to manually test many different sets of hyperparameters to select the best set. Thankfully, we can use the caret package's expand.grid and train functions to automate this process for us. We can pass in several sets of hyperparameters and the train function will run every possible set of parameters. We must pass in 'xgbTree' to select the XGBoost model and pass in 'ROC' as our evaluating metric. We also passed in 'cv' and 10 into the trainControl function to designate our metric be tested using 10-fold cross-validation.

# Create search grid of hyperparameters to tune.
grid <- expand.grid(nrounds   = c(300, 400, 500, 600),
                    max_depth = c(3, 5, 7, 9),
                    eta       = c(0.05, 0.1, 0.2, 0.3),
                    gamma     = 1,
                    colsample_bytree = c(0.5, 1),
                    min_child_weight = 1,
                    subsample = 1)

# 10 fold Cross-validation
ctrl <- trainControl(method = "cv",
                     number = 10,
                     summaryFunction = twoClassSummary,
                     classProbs      = TRUE,
                     allowParallel   = TRUE)

set.seed(1234) # Set the seed to create reproducible train and test sets.
system.time(xgbTune <- train(x = dtrain,
                             y = factor(make.names(train.label)),
                             method    = "xgbTree",
                             metric    = "ROC",
                             tuneGrid  = grid,
                             verbose   = TRUE,
                             trControl = ctrl))
     user   system  elapsed 
  652.02     2.64 35144.33 
Visualize Hyperparameters Grid¶
Now we can print out our results and visualize each set of hyperparameters which will provide us with an easy selection of the best performing set.

# Plot tuning results.
xgbTune

ggplot(xgbTune) +
  theme(legend.position="top")
 eXtreme Gradient Boosting 

No pre-processing
Resampling: Cross-Validated (10 fold)
Tuning parameter 'gamma' was held constant at a value of 1
Tuning
 parameter 'min_child_weight' was held constant at a value of 1

Tuning parameter 'subsample' was held constant at a value of 1
ROC was used to select the optimal model using  the largest value.
The final values used for the model were nrounds = 400, max_depth = 5, eta
 = 0.1, gamma = 1, colsample_bytree = 1, min_child_weight = 1 and subsample = 1. 

In the output of the printout, it states that the final values used for the model were nrounds = 400, max_depth = 5, eta = 0.1, gamma = 1, colsample_bytree = 1, min_child_weight = 1 and subsample = 1. These hyperparameters produced the highest ROC score, and are the hyperparameters we'll use when we train our model in the next step.

Train Model Using Tuned Hyperparameters¶
Now that we know our optimal hyperparameters, its time to train our model using the XGBoost package. We'll pass in our parameters and pass in our sparse training matrix dtrain and label vector train.label to the xgboost function.

param <- list(objective   = 'binary:logistic',
              eval_metric = 'error',
              max_depth   = 5,
              eta         = 0.1,
              gammma      = 1,
              colsample_bytree = 1,
              min_child_weight = 1)

set.seed(1234) # Set the seed to create reproducible train and test sets.
system.time(xgb <- xgboost(params  = param,
                           data    = dtrain,
                           label   = train.label, 
                           nrounds = 400,
                           print_every_n = 100,
                           verbose = 1))
 [1]    train-error:0.070620 
[101]   train-error:0.056590 
[201]   train-error:0.053395 
[301]   train-error:0.051414 
[400]   train-error:0.050207 
   user  system elapsed 
 137.39    1.18   21.75 
Determine Which Features Are Most Important¶
After the model has finished training, we can view which features had the greatest impact on predictive performance.

# Get the trained model.
model <- xgb.dump(xgb, with_stats=TRUE)

# Get the feature real names.
names <- dimnames(dtrain)[[2]]

# Compute feature importance matrix.
importance_matrix <- xgb.importance(names, model=xgb)[0:20] # View top 20 most important features

# Plot
xgb.plot.importance(importance_matrix)
 
We can see that creating features at the clinic level provides the best results. The clinic no-show rate, new patient to clinic, and clinic cancel rate are the top three most important features. The model gained a substantial amount of improvement after narrowing each from the patient level down to the clinic level.

Evaluate Model Performance With Confusion Matrix¶
The Confusion Matrix provides us with a large amount of information regarding the performance of our model. But first, we must choose a threshold. The threshold value is the cutoff at which we determine a positive prediction and a negative prediction. Down below, we have chosen 0.34 as our cutoff threshold. The model produces a probability for each prediction and we convert it to either a 1 (no-show) if the probability is above or equal to 0.34 and a 0 (show) if the probability is below 0.34.

# Create confusion matrix using the caret package.
pred <- predict(xgb, dtest)
pred.resp <- ifelse(pred >= 0.34, 1, 0)
confusionMatrix(pred.resp, test.label, positive='1')
 Confusion Matrix and Statistics

          Reference
Prediction     0     1
         0 79559  2393
         1  3597 11052
                                          
               Accuracy : 0.938           
                 95% CI : (0.9365, 0.9395)
    No Information Rate : 0.8608          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.7506          
 Mcnemar's Test P-Value : < 2.2e-16       
                                          
            Sensitivity : 0.8220          
            Specificity : 0.9567          
         Pos Pred Value : 0.7545          
         Neg Pred Value : 0.9708          
             Prevalence : 0.1392          
         Detection Rate : 0.1144          
   Detection Prevalence : 0.1516          
      Balanced Accuracy : 0.8894          
                                          
       'Positive' Class : 1               
                                          
We can see that our overall accuracy score is 0.938. This means it will accurately predict whether a patient will either show or no-show 93.8% of the time.

However, as stated earlier, we're more interested in how well it performs while predicting which patients will no-show rather than show. We can see our sensitivity score is 0.8220, which means that our true positive rate is 82.2%. We can confidently predict 82.2% of the time which patients will no-show their appointment.

However, we do still care about our false positive rate as well, as we don't want our technicians wasting work hours on patients that are wrongly predicted to no-show. We can see that our specificity score is 0.9567. This means that our true negative rate is 95.6%, or that we can accurately predict 95.6% of the time which patients will show up. Another more common way to look at it is to take its inverse and say that we have a false positive rate of 4.4%.

The next important statistic we use to measure our model's performance is Cohen's Kappa Coefficient. We currently have a score of 0.7506. A score < 0 indicates no agreement and 0–0.20 as slight, 0.21–0.40 as fair, 0.41–0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1 as almost perfect agreement. Another guideline characterizes kappa scores over 0.75 as excellent, 0.40 to 0.75 as fair to good, and below 0.40 as poor. You can read more about the Kappa score at https://en.wikipedia.org/wiki/Cohen%27s_kappa.

Plotting The ROC To View Various Thresholds¶
An ROC curve allows us to visualize our model's performance when selecting different thresholds. The threshold value is indicated by the dots on the curved line. Each dot lets us view the average true positive rate and average false positive rate for each threshold. As the threshold value gets lower, the average true positive rate gets higher. However, the average false positive rate gets higher as well. It's important to select a threshold that provides an acceptable true positive rate while also limiting the false positive rate. You can read more at https://en.wikipedia.org/wiki/Receiver_operating_characteristic.

# Use ROCR package to plot ROC Curve
xgb.pred <- prediction(pred, test.label)
xgb.perf <- performance(xgb.pred, "tpr", "fpr")

plot(xgb.perf,
     avg="threshold",
     colorize=TRUE,
     lwd=3,
     print.cutoffs.at=seq(0, 1, by=0.05),
     text.adj=c(-0.5, 0.5),
     text.cex=0.6)
grid(col="lightgray")
axis(1, at=seq(0, 1, by=0.1))
axis(2, at=seq(0, 1, by=0.1))
abline(v=c(0.1, 0.3, 0.5, 0.7, 0.9), col="lightgray", lty="dotted")
abline(h=c(0.1, 0.3, 0.5, 0.7, 0.9), col="lightgray", lty="dotted")
lines(x=c(0, 1), y=c(0, 1), col="black", lty="dotted")
 
Our goal was to achieve a minimum true positive rate (sensitivity) of 80% while maintaining a minimum true negative rate (specificity) of 95%. As we can see from the ROC curve above, setting our threshold to 0.34 will give us our desired goal with a true positive rate of 82.2% and a true negative rate of 95.6%. However, if management decides they would like a higher true positive rate and deem a higher false positive rate as acceptable, a threshold of 0.22 could be used which would provide a true positive rate of 0.9026 (90.2%) and true negative rate of 0.9220 (92.2%).

End User Web Application¶
I developed the below JavaScript web application as a simple intuitive solution to provide agency management and staff with a way to quickly locate records pertinent to their needs. The application can load any comma separated value (CSV) file into a table that can then be filtered using any search term(s) or by using custom #hashtags. Using R, the data is pulled from the data warehouse and no-show prediction scores are generated using the XGBoost model for all future appointments. The CSV file is then preprocessed in R to add styling (HTML, CSS) and #hashtags to specific records using conditional logic. Under the hood, the web application uses regular expressions (REGEX) that are input by the user to locate every row in the table where it finds a match. The user can use multiple search terms separated by commas, as in (this OR that), or limit the scope of their search by locating records that meet multiple criteria as in (this OR that) AND (this OR that). The user can also add terms s/he would like to exclude from the results. Regular expressions give the user extreme flexibility and precision to locate any record(s) in the table.

Below is a lite version of the web application that can filter any records displayed in the table. Each row contains a patient's appointment that has a no-show prediction score above the 0.34 threshold. Each row contains the appointment's department, clinic name within the department, no-show prediction score, patient's information (name, last four of SSN, and phone numbers), and appointment date/time.

As a hands-on demo, start typing "infect", which will automatically start typing in the first search field. Then, select "Infectious Disease" from the drop-down menu, which will convert it to the department's 3-digit identifier code and then either press Enter or click Search. 15 no-show predictions will be displayed all belonging to the Infectious Disease department.

Next, let's narrow our results to the patients most likely to no-show. Either click the "Highly Likely" tag or drag-and-drop it into the second search field and press Enter again. Now, only the two patients with a prediction score greater than 0.80 in the Infectious Disease department are displayed.